{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f758013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import random\n",
    "import pytesseract\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3fe375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3999a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/chiragsaipanuganti/morph?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 303M/303M [00:35<00:00, 8.99MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "#dataset_path = kagglehub.dataset_download(\"jessicali9530/lfw-dataset\")\n",
    "dataset_path = kagglehub.dataset_download(\"chiragsaipanuganti/morph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f68a3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffdb5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.jpg')):\n",
    "            image_paths.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595db6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_whitecard(image_paths, naming, height=1540, width=1754, im_width=250, im_height=250, spacing=80):\n",
    "    whitecard = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "    x = 10\n",
    "    y = 10\n",
    "\n",
    "    diagonal_size = int(np.sqrt(im_width**2 + im_height**2)) + 10\n",
    "\n",
    "    for i, name in zip(image_paths, naming):\n",
    "        img = cv2.imread(i)\n",
    "        img = cv2.resize(img, (im_width, im_height))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        rot_canvas = np.ones((diagonal_size, diagonal_size, 3), dtype=np.uint8) * 255\n",
    "        start_x = (diagonal_size - im_width) // 2\n",
    "        start_y = (diagonal_size - im_height) // 2\n",
    "        rot_canvas[start_y:start_y + im_height, start_x:start_x + im_width] = img\n",
    "\n",
    "        rot_angle = random.uniform(-180, 180)\n",
    "        center = (diagonal_size // 2, diagonal_size // 2)\n",
    "        rot_matrix = cv2.getRotationMatrix2D(center, rot_angle, 1.0)\n",
    "        rot_img = cv2.warpAffine(rot_canvas, rot_matrix, (diagonal_size, diagonal_size), borderValue=(255, 255, 255))\n",
    "\n",
    "        whitecard[y:y+diagonal_size, x:x+diagonal_size] = rot_img\n",
    "\n",
    "        cv2.putText(whitecard, str(name), (x + diagonal_size // 2, y + diagonal_size + (spacing // 2)), \n",
    "                   cv2.FONT_HERSHEY_SCRIPT_COMPLEX, 1.3, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        x += diagonal_size + 40\n",
    "        if x > (width-diagonal_size):\n",
    "            x = 10\n",
    "            y += diagonal_size + spacing\n",
    "        if y > (height-diagonal_size):\n",
    "            return whitecard\n",
    "    return whitecard\n",
    "\n",
    "def get_cards(image_paths, num, im_width, im_height):\n",
    "    cards = []\n",
    "    random.shuffle(image_paths)\n",
    "    for i in range(num):\n",
    "        card = populate_whitecard(image_paths[i*20:(i+1)*20], [f\"{i*20+j+1}\" for j in range(20)], im_height=im_height, im_width=im_width)\n",
    "        cards.append(card)\n",
    "    return cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e2aef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_whitecard_labels(image_paths, naming, height=1540, width=1754, im_width=250, im_height=250, spacing=80):\n",
    "    whitecard = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "    x = 10\n",
    "    y = 10\n",
    "\n",
    "    diagonal_size = int(np.sqrt(im_width**2 + im_height**2)) + 10\n",
    "    label_entries = []\n",
    "\n",
    "    for i, name in zip(image_paths, naming):\n",
    "        img = cv2.imread(i)\n",
    "        img = cv2.resize(img, (im_width, im_height))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        rot_canvas = np.ones((diagonal_size, diagonal_size, 3), dtype=np.uint8) * 255\n",
    "        start_x = (diagonal_size - im_width) // 2\n",
    "        start_y = (diagonal_size - im_height) // 2\n",
    "        rot_canvas[start_y:start_y + im_height, start_x:start_x + im_width] = img\n",
    "\n",
    "        rot_angle = random.uniform(-180, 180)\n",
    "        center = (diagonal_size // 2, diagonal_size // 2)\n",
    "        rot_matrix = cv2.getRotationMatrix2D(center, rot_angle, 1.0)\n",
    "        rot_img = cv2.warpAffine(rot_canvas, rot_matrix, (diagonal_size, diagonal_size), borderValue=(255, 255, 255))\n",
    "\n",
    "        whitecard[y:y+diagonal_size, x:x+diagonal_size] = rot_img\n",
    "\n",
    "        cv2.putText(whitecard, str(name), (x + diagonal_size // 2, y + diagonal_size + (spacing // 2)), \n",
    "                   cv2.FONT_HERSHEY_SCRIPT_COMPLEX, 1.3, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        box_x_center = (x + diagonal_size / 2) / width\n",
    "        box_y_center = (y + diagonal_size / 2 + spacing / 2) / height\n",
    "        box_width = diagonal_size / width\n",
    "        box_height = (diagonal_size + spacing) / height\n",
    "        label_entries.append(f\"0 {box_x_center:.6f} {box_y_center:.6f} {box_width:.6f} {box_height:.6f}\")\n",
    "\n",
    "        x += diagonal_size + 40\n",
    "        if x > (width-diagonal_size):\n",
    "            x = 10\n",
    "            y += diagonal_size + spacing\n",
    "        if y > (height-diagonal_size):\n",
    "            break\n",
    "    return whitecard, label_entries\n",
    "\n",
    "def get_cards_labels(image_paths, num, im_width, im_height, output_path):\n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    labels_path = output_path / \"labels\"\n",
    "    labels_path.mkdir(exist_ok=True)\n",
    "    cards = []\n",
    "\n",
    "    random.shuffle(image_paths)\n",
    "    for i in range(num):\n",
    "        imgs = image_paths[i*20:(i+1)*20]\n",
    "        names = [f\"{i*20+j+1}\" for j in range(len(imgs))]\n",
    "        card, labels = populate_whitecard_labels(imgs, names, im_width=im_width, im_height=im_height)\n",
    "        filename = f\"card_{i+1}\"\n",
    "        \n",
    "        card_bgr = cv2.cvtColor(card, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(str(output_path / f\"{filename}.jpg\"), card_bgr)\n",
    "\n",
    "        with open(labels_path / f\"{filename}.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(labels))\n",
    "    return cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e69643e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cards_labels(image_paths, num=100, im_width=200, im_height=200, output_path=\"validation_cards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9949717",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11n.pt')  # nano\n",
    "model = YOLO('yolo11s.pt')  # small\n",
    "model = YOLO('yolo11m.pt')  # medium\n",
    "model = YOLO('yolo11l.pt')  # large\n",
    "model = YOLO('yolo11x.pt')  # extra large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "325a5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11n-pose.pt')  # nano\n",
    "model = YOLO('yolo11s-pose.pt')  # small\n",
    "model = YOLO('yolo11m-pose.pt')  # medium\n",
    "model = YOLO('yolo11l-pose.pt')  # large\n",
    "model = YOLO('yolo11x-pose.pt')  # extra large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3c45c61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.146 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.144 🚀 Python-3.11.9 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4070 Ti SUPER, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=person_entry.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11x.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=person_entry_model3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/person_entry_model3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n",
      "  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n",
      "  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      "  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n",
      "  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      "  6                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
      "  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      "  8                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
      "  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]                 \n",
      " 10                  -1  2   3264768  ultralytics.nn.modules.block.C2PSA           [768, 768, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2   1700352  ultralytics.nn.modules.block.C3k2            [1536, 384, 2, True]          \n",
      " 17                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   5317632  ultralytics.nn.modules.block.C3k2            [1152, 768, 2, True]          \n",
      " 20                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
      " 23        [16, 19, 22]  1   3146707  ultralytics.nn.modules.head.Detect           [1, [384, 768, 768]]          \n",
      "YOLO11x summary: 357 layers, 56,874,931 parameters, 56,874,915 gradients, 195.4 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 7202.9±2468.4 MB/s, size: 414.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /repos/uni-si/R2S4/DNN/End_project/labels/train... 100 images, 0 backgrounds, 0 corrupt: 100%|██████████| 100/100 [00:00<00:00, 1126.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /repos/uni-si/R2S4/DNN/End_project/labels/train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2731.9±2230.4 MB/s, size: 415.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /repos/uni-si/R2S4/DNN/End_project/labels/test... 100 images, 0 backgrounds, 0 corrupt: 100%|██████████| 100/100 [00:00<00:00, 953.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /repos/uni-si/R2S4/DNN/End_project/labels/test.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/person_entry_model3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0005), 173 bias(decay=0.0)\n",
      "Image sizes 1024 train, 1024 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/person_entry_model3\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100      37.8G       1.95      1.912       2.46         85       1024: 100%|██████████| 13/13 [01:39<00:00,  7.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:20<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100       2000      0.983      0.992      0.995       0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100      37.4G      0.926     0.7484      1.307        154       1024: 100%|██████████| 13/13 [02:12<00:00, 10.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [01:11<00:00, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100       2000          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100      37.3G     0.9348     0.7083      1.354        214       1024: 100%|██████████| 13/13 [02:09<00:00,  9.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  29%|██▊       | 2/7 [00:12<00:31,  6.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo11x.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# lub 'yolov8n.pt' jeśli v11 jeszcze nie ma\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Trenuj na swoich danych\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperson_entry.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperson_entry_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/engine/model.py:796\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/engine/trainer.py:227\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/engine/trainer.py:459\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mval \u001b[38;5;129;01mor\u001b[39;00m final_epoch \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopper\u001b[38;5;241m.\u001b[39mpossible_stop \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitness \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_metrics(metrics\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_loss_items(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr})\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopper(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitness) \u001b[38;5;129;01mor\u001b[39;00m final_epoch\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/engine/trainer.py:656\u001b[0m, in \u001b[0;36mBaseTrainer.validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    649\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m    Run validation on test set using self.validator.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m        fitness (float): Fitness score for the validation.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 656\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m     fitness \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfitness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# use loss as fitness measure if not found\u001b[39;00m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_fitness \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_fitness \u001b[38;5;241m<\u001b[39m fitness:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/engine/validator.py:213\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m    210\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(batch)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/utils/ops.py:61\u001b[0m, in \u001b[0;36mProfile.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stop timing.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart  \u001b[38;5;66;03m# delta-time\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/utils/ops.py:71\u001b[0m, in \u001b[0;36mProfile.time\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get current time with CUDA synchronization if applicable.\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda:\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/cuda/__init__.py:985\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    983\u001b[0m _lazy_init()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolo11x.pt')\n",
    "\n",
    "model.train(\n",
    "    data='person_entry.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=1024,\n",
    "    batch=8,\n",
    "    name='person_entry_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "88ef3d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba obrazów: 100\n",
      "Liczba labeli: 100\n",
      "Obrazy bez labeli: set()\n",
      "Labeli bez obrazów: set()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "img_dir = \"validation_cards\"\n",
    "label_dir = os.path.join(img_dir, \"labels\")\n",
    "\n",
    "imgs = sorted([f.strip() for f in os.listdir(img_dir) if f.lower().endswith(\".jpg\")])\n",
    "labels = sorted([f.strip() for f in os.listdir(label_dir) if f.lower().endswith(\".txt\")])\n",
    "\n",
    "print(\"Liczba obrazów:\", len(imgs))\n",
    "print(\"Liczba labeli:\", len(labels))\n",
    "\n",
    "img_base = [os.path.splitext(f)[0].lower() for f in imgs]\n",
    "label_base = [os.path.splitext(f)[0].lower() for f in labels]\n",
    "\n",
    "set_img = set(img_base)\n",
    "set_label = set(label_base)\n",
    "\n",
    "print(\"Obrazy bez labeli:\", set_img - set_label)\n",
    "print(\"Labeli bez obrazów:\", set_label - set_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2660841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardProcessor:\n",
    "    def __init__(self, detection_model=\"yolo11x.pt\", pose_model=\"yolo11x-pose.pt\"):\n",
    "        self.detection_model = YOLO(detection_model)\n",
    "        self.pose_model = YOLO(pose_model)\n",
    "\n",
    "        self.debug_ocr_path = Path(\"debug_ocr_images\")\n",
    "        self.debug_ocr_path.mkdir(exist_ok=True)\n",
    "        self.ocr_debug_counter = 0\n",
    "\n",
    "        try:\n",
    "            self.trocr_processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1', use_fast=True)\n",
    "            self.trocr_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\n",
    "            if torch.cuda.is_available() and self.trocr_model is not None:\n",
    "                self.trocr_model.to('cuda')\n",
    "            print(\"TrOCR model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading TrOCR model: {e}. TrOCR will not be available.\")\n",
    "            self.trocr_processor = None\n",
    "            self.trocr_model = None\n",
    "    \n",
    "    def process_card(self, card_path):\n",
    "        card = cv2.imread(card_path)\n",
    "\n",
    "        image_regions = self.detect_image_regions(card)\n",
    "\n",
    "        print(f\"Found {len(image_regions)} image regions.\")\n",
    "\n",
    "        results = []\n",
    "        valid_count = 0\n",
    "        for i, region in enumerate(image_regions):\n",
    "            crop = self.extract_crop(card, region)\n",
    "            if crop.shape[0] < 50 or crop.shape[1] < 50:\n",
    "                continue\n",
    "        \n",
    "            rotated_crop, angle = self.correct_orientation(crop)\n",
    "        \n",
    "            is_valid = self.validate_photo(rotated_crop)\n",
    "\n",
    "            if is_valid:\n",
    "                valid_count += 1\n",
    "                id = self.find_id(card, region)\n",
    "\n",
    "                results.append({\n",
    "                    'id': id,\n",
    "                    'rotated_crop': rotated_crop,\n",
    "                    'angle': angle,\n",
    "                    'source_card': Path(card_path).name,\n",
    "                    'region_idx': i,\n",
    "                    'region': region\n",
    "                })\n",
    "        print(f\"Valid faces found: {valid_count}/{len(image_regions)}\")\n",
    "        return results\n",
    "    \n",
    "    def detect_image_regions(self, card):\n",
    "        # This method will now primarily use YOLO to detect image content.\n",
    "        # Padding will be added to YOLO detections to make them less tightly cropped.\n",
    "        \n",
    "        yolo_detected_content_regions = []\n",
    "        yolo_results = self.detection_model(card, classes=[0], verbose=False) # classes=[0] for person/face\n",
    "        \n",
    "        # Define padding percentage (e.g., 15% added to each side, so total increase is 30% of original)\n",
    "        # Or, simpler: increase width and height by a factor. Let's try increasing w and h by 20%.\n",
    "        padding_factor_w = 1.20 # Increase width by 20%\n",
    "        padding_factor_h = 1.20 # Increase height by 20%\n",
    "\n",
    "        card_h_for_bounds, card_w_for_bounds = card.shape[:2]\n",
    "\n",
    "        for result in yolo_results:\n",
    "            if result.boxes is not None:\n",
    "                for box in result.boxes:\n",
    "                    if box.conf[0] > 0.3: \n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        \n",
    "                        orig_x, orig_y = int(x1), int(y1)\n",
    "                        orig_w, orig_h = int(x2 - x1), int(y2 - y1)\n",
    "\n",
    "                        if orig_w <= 0 or orig_h <= 0: # Skip invalid original detections\n",
    "                            continue\n",
    "\n",
    "                        # Calculate new dimensions with padding\n",
    "                        padded_w = int(orig_w * padding_factor_w)\n",
    "                        padded_h = int(orig_h * padding_factor_h)\n",
    "\n",
    "                        # Calculate new top-left coordinates, keeping the center\n",
    "                        # delta_w = padded_w - orig_w\n",
    "                        # delta_h = padded_h - orig_h\n",
    "                        # new_x = orig_x - delta_w // 2\n",
    "                        # new_y = orig_y - delta_h // 2\n",
    "                        \n",
    "                        # Simpler: calculate center, then new x1,y1 from new w,h\n",
    "                        center_x = orig_x + orig_w / 2\n",
    "                        center_y = orig_y + orig_h / 2\n",
    "\n",
    "                        new_x = int(round(center_x - padded_w / 2))\n",
    "                        new_y = int(round(center_y - padded_h / 2))\n",
    "                        \n",
    "                        # Ensure padded region is within card boundaries\n",
    "                        final_x = max(0, new_x)\n",
    "                        final_y = max(0, new_y)\n",
    "                        final_w = min(padded_w, card_w_for_bounds - final_x)\n",
    "                        final_h = min(padded_h, card_h_for_bounds - final_y)\n",
    "\n",
    "                        if final_w > 0 and final_h > 0: \n",
    "                            yolo_detected_content_regions.append((final_x, final_y, final_w, final_h))\n",
    "                            \n",
    "        print(f\"YOLO detected and padded {len(yolo_detected_content_regions)} raw content regions.\")\n",
    "\n",
    "        # Deduplicate regions detected by YOLO\n",
    "        unique_yolo_regions = self.remove_duplicate_regions(yolo_detected_content_regions, iou_threshold=0.4) \n",
    "        print(f\"Unique content regions after YOLO deduplication: {len(unique_yolo_regions)}\")\n",
    "        \n",
    "        # Filter these content regions using your existing filter_valid_regions method\n",
    "        # The filter_valid_regions parameters (min_expected_content_dimension = 180, \n",
    "        # max_expected_content_dimension = 330) should be appropriate for these padded regions.\n",
    "        # Original content ~200-283. Padded by 1.2 factor -> ~240-340. This range is mostly\n",
    "        # covered by [180,330], though the upper bound might clip some slightly.\n",
    "        # If issues persist, max_expected_content_dimension might need a slight increase.\n",
    "        valid_content_regions = self.filter_valid_regions(unique_yolo_regions, card.shape)\n",
    "        print(f\"Valid content regions after filtering: {len(valid_content_regions)}\")\n",
    "\n",
    "        final_regions_sorted = sorted(valid_content_regions, key=lambda r: (r[1], r[0]))\n",
    "        \n",
    "        return final_regions_sorted\n",
    "    \n",
    "    def remove_duplicate_regions(self, regions, iou_threshold=0.4):\n",
    "        if not regions:\n",
    "            return []\n",
    "        \n",
    "        regions_list = list(regions)\n",
    "        unique_regions = []\n",
    "\n",
    "        while regions_list:\n",
    "            current_region = regions_list.pop(0) \n",
    "            unique_regions.append(current_region)\n",
    "\n",
    "            remaining_regions_after_check = []\n",
    "            for region_to_compare in regions_list:\n",
    "                iou = self.calculate_iou(current_region, region_to_compare)\n",
    "                if iou <= iou_threshold: \n",
    "                    remaining_regions_after_check.append(region_to_compare)\n",
    "\n",
    "            regions_list = remaining_regions_after_check \n",
    "            \n",
    "        return unique_regions\n",
    "    \n",
    "    def calculate_iou(self, region1, region2):\n",
    "        x1, y1, w1, h1 = region1\n",
    "        x2, y2, w2, h2 = region2\n",
    "\n",
    "        x_intersect = max(x1, x2)\n",
    "        y_intersect = max(y1, y2)\n",
    "        w_intersect = min(x1 + w1, x2 + w2)\n",
    "        h_intersect = min(y1 + h1, y2 + h2)\n",
    "\n",
    "        if w_intersect <= x_intersect or h_intersect <= y_intersect:\n",
    "            return 0\n",
    "        \n",
    "        intersection = (w_intersect - x_intersect) * (h_intersect - y_intersect)\n",
    "\n",
    "        area1 = w1 * h1\n",
    "        area2 = w2 * h2\n",
    "\n",
    "        if area1 == 0 or area2 == 0:\n",
    "            return 0\n",
    "        \n",
    "        iou = intersection / float(area1 + area2 - intersection)\n",
    "\n",
    "        return iou\n",
    "    \n",
    "    def filter_valid_regions(self, regions, card_shape):\n",
    "        valid_regions = []\n",
    "        card_h, card_w = card_shape[:2]\n",
    "        min_size = 100\n",
    "        max_size = min(card_h, card_w) // 3\n",
    "\n",
    "        for x, y, w, h in regions:\n",
    "            aspect_ratio = max(w, h) / min(w, h) if min(w,h) > 0 else 0 \n",
    "            if aspect_ratio > 1.5:\n",
    "                continue\n",
    "            if min_size <= min(w, h) and max(w, h) <= max_size:\n",
    "                margin = 5\n",
    "                if (margin < x < card_w - w - margin and\n",
    "                    margin < y < card_h - h - margin):\n",
    "                    valid_regions.append((x, y, w, h))\n",
    "\n",
    "        return valid_regions\n",
    "    \n",
    "    def find_id(self, card, region):\n",
    "        x, y, w, h = region\n",
    "\n",
    "        search_areas = []\n",
    "\n",
    "        below_y = y + h\n",
    "        below_h = min(80, card.shape[0] - below_y)\n",
    "        search_x = max(0, x + 10)\n",
    "        search_w = min(card.shape[1] - search_x, w + 40)\n",
    "\n",
    "        if below_y + below_h <= card.shape[0] and search_x + search_w <= card.shape[1] and below_h > 0 and search_w > 0:\n",
    "            search_areas.append(('below', (search_x, below_y, search_w, below_h)))\n",
    "        \n",
    "        #above_h = min(60, y)\n",
    "        #if above_h > 10:\n",
    "        #    search_areas.append(('above', (x, max(0, y - above_h), w, above_h)))\n",
    "\n",
    "        #right_x = x + w + 2\n",
    "        #right_w = min(100, card.shape[1] - right_x)\n",
    "        #if right_x + right_w <= card.shape[1]:\n",
    "        #    search_areas.append(('right', (right_x, y, right_w, h)))\n",
    "        \n",
    "        #left_w = min(100, x)\n",
    "        #if left_w > 10:\n",
    "        #    search_areas.append(('left', (max(0, x - left_w), y, left_w, h)))\n",
    "\n",
    "        for location, (sx, sy, sw, sh) in search_areas:\n",
    "            id = self.extract_id(card, (sx, sy, sw, sh))\n",
    "\n",
    "            if id not in [\"ID_NOT_FOUND\", \"ID_NOT_DETECTED\", \"OCR_ERROR\"]:\n",
    "                return id\n",
    "            \n",
    "        return \"ID_NOT_DETECTED\"\n",
    "    \n",
    "    def extract_id(self, card, region):\n",
    "        if self.trocr_model is None or self.trocr_processor is None:\n",
    "            print(\"Debug - TrOCR model not initialized, skipping OCR.\")\n",
    "            return \"OCR_ERROR\"\n",
    "        \n",
    "        x, y, w, h = region\n",
    "\n",
    "        x = max(0, x)\n",
    "        y = max(0, y)\n",
    "        w = min(w, card.shape[1] - x)\n",
    "        h = min(h, card.shape[0] - y)\n",
    "\n",
    "        if w <= 0 or h <= 0:\n",
    "            print(f\"Debug - Invalid crop dimensions for OCR: w={w}, h={h}\")\n",
    "            return \"ID_NOT_FOUND\"\n",
    "        \n",
    "        id_crop = card[y:y+h, x:x+w]\n",
    "\n",
    "        if id_crop.size == 0:\n",
    "            print(f\"Debug - Empty crop for OCR for region {region}\")\n",
    "            return \"ID_NOT_FOUND\"\n",
    "        \n",
    "        min_crop_dim_for_ocr = 10\n",
    "        if id_crop.shape[0] < min_crop_dim_for_ocr or id_crop.shape[1] < min_crop_dim_for_ocr:\n",
    "            print(f\"Debug - OCR crop too small: {id_crop.shape}\")\n",
    "            return \"ID_NOT_DETECTED\"\n",
    "        \n",
    "        ocr_attempt_filename_prefix = f\"ocr_input_{self.ocr_debug_counter}\"\n",
    "        self.ocr_debug_counter += 1\n",
    "\n",
    "        try:\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(id_crop, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            pixel_values = self.trocr_processor(images=pil_image, return_tensors=\"pt\").pixel_values\n",
    "            if torch.cuda.is_available():\n",
    "                 pixel_values = pixel_values.to(self.trocr_model.device)\n",
    "\n",
    "            generated_ids = self.trocr_model.generate(pixel_values, max_length=10) \n",
    "\n",
    "            decoded_texts = self.trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            if not decoded_texts or not decoded_texts[0].strip():\n",
    "                debug_filename = self.debug_ocr_path / f\"{ocr_attempt_filename_prefix}_fails_empty.png\"\n",
    "                cv2.imwrite(str(debug_filename), id_crop)\n",
    "                print(f\"Debug - Saved empty/whitespace OCR crop to {debug_filename}\")\n",
    "                return \"ID_NOT_DETECTED\"\n",
    "\n",
    "            id_text_from_ocr = decoded_texts[0].strip()\n",
    "\n",
    "            numeric_id_text = \"\".join(filter(str.isdigit, id_text_from_ocr))\n",
    "\n",
    "            if not numeric_id_text:\n",
    "                debug_filename = self.debug_ocr_path / f\"{ocr_attempt_filename_prefix}_fails_nonumeric_{id_text_from_ocr.replace(' ','_')}.png\"\n",
    "                cv2.imwrite(str(debug_filename), id_crop)\n",
    "                print(f\"Debug - Saved non-numeric OCR crop to {debug_filename} (Raw: '{id_text_from_ocr}')\")\n",
    "                return \"ID_NOT_DETECTED\"\n",
    "            \n",
    "            print(f\"Debug - TrOCR extracted ID: {numeric_id_text} from region {region}\")\n",
    "            return numeric_id_text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Debug - Exception during TrOCR processing in extract_id for region {region}: {e}\")\n",
    "            return \"OCR_ERROR\"\n",
    "        \n",
    "    def correct_orientation(self, crop):\n",
    "        best_crop = crop.copy()\n",
    "        best_angle = 0\n",
    "        best_score = 0\n",
    "\n",
    "        angles = list(range(0, 360, 15))\n",
    "\n",
    "        for angle in angles:\n",
    "            if angle == 0:\n",
    "                rotated = crop\n",
    "            else:\n",
    "                rotated = self.rotate_image(crop, angle)\n",
    "\n",
    "            orientation_data = self.analyze_face_orientation(rotated)\n",
    "\n",
    "            if orientation_data['has_face']:\n",
    "                score = self.calculate_orientation_score(orientation_data)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_crop = rotated.copy()\n",
    "                    best_angle = angle\n",
    "\n",
    "        if best_angle > 0:\n",
    "            angles = list(range(max(0, best_angle - 10), min(360, best_angle + 11), 1))\n",
    "\n",
    "            for angle in angles:\n",
    "                rotated = self.rotate_image(crop, angle)\n",
    "                orientation_data = self.analyze_face_orientation(rotated)\n",
    "\n",
    "                if orientation_data['has_face']:\n",
    "                    score = self.calculate_orientation_score(orientation_data)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_crop = rotated.copy()\n",
    "                        best_angle = angle\n",
    "\n",
    "        return best_crop, best_angle\n",
    "\n",
    "    def analyze_face_orientation(self, image):\n",
    "\n",
    "        try:\n",
    "            results = self.pose_model(image, verbose=False)\n",
    "\n",
    "            orientation_data = {\n",
    "                'has_face': False,\n",
    "                'confidence': 0,\n",
    "                'face_upright': False,\n",
    "                'keypoints': {},\n",
    "            }\n",
    "\n",
    "            for result in results:\n",
    "                keypoints = result.keypoints\n",
    "                boxes = result.boxes\n",
    "\n",
    "                if keypoints is not None and boxes is not None and len(boxes) > 0:\n",
    "                    if boxes.conf.is_cuda:\n",
    "                        best_idx = torch.argmax(boxes.conf).item()\n",
    "                    else:\n",
    "                        best_idx = np.argmax(boxes.conf.cpu().numpy()).item()\n",
    "\n",
    "                    kp = keypoints[best_idx].cpu()\n",
    "                    box_conf = boxes.conf[best_idx].cpu().item()\n",
    "\n",
    "                    nose_tensor = kp.xy[0][0]\n",
    "                    left_eye_tensor = kp.xy[0][1]\n",
    "                    right_eye_tensor = kp.xy[0][2]\n",
    "\n",
    "                    nose = nose_tensor.tolist() if torch.is_tensor(nose_tensor) and nose_tensor.numel() == 2 else [0,0]\n",
    "                    left_eye = left_eye_tensor.tolist() if torch.is_tensor(left_eye_tensor) and left_eye_tensor.numel() == 2 else [0,0]\n",
    "                    right_eye = right_eye_tensor.tolist() if torch.is_tensor(right_eye_tensor) and right_eye_tensor.numel() == 2 else [0,0]\n",
    "\n",
    "                    valid_keypoints = 0\n",
    "                    if nose[0] > 0 and nose[1] > 0:\n",
    "                        orientation_data['keypoints']['nose'] = nose\n",
    "                        valid_keypoints += 1\n",
    "\n",
    "                    if left_eye[0] > 0 and left_eye[1] > 0:\n",
    "                        orientation_data['keypoints']['left_eye'] = left_eye\n",
    "                        valid_keypoints += 1\n",
    "\n",
    "                    if right_eye[0] > 0 and right_eye[1] > 0:\n",
    "                        orientation_data['keypoints']['right_eye'] = right_eye\n",
    "                        valid_keypoints +=1\n",
    "\n",
    "                    if valid_keypoints >= 2:\n",
    "                        orientation_data['has_face'] = True\n",
    "                        orientation_data['confidence'] = box_conf\n",
    "\n",
    "                        if 'nose' in orientation_data['keypoints'] and ('left_eye' in orientation_data['keypoints'] or 'right_eye' in orientation_data['keypoints']):\n",
    "                            nose_y = orientation_data['keypoints']['nose'][1]\n",
    "                            \n",
    "                            if 'left_eye' in orientation_data['keypoints'] and 'right_eye' in orientation_data['keypoints']:\n",
    "                                avg_eye_y = (orientation_data['keypoints']['left_eye'][1] + orientation_data['keypoints']['right_eye'][1]) / 2\n",
    "                                orientation_data['face_upright'] = avg_eye_y < nose_y\n",
    "                            elif 'left_eye' in orientation_data['keypoints']:\n",
    "                                orientation_data['face_upright'] = orientation_data['keypoints']['left_eye'][1] < nose_y\n",
    "                            elif 'right_eye' in orientation_data['keypoints']:\n",
    "                                orientation_data['face_upright'] = orientation_data['keypoints']['right_eye'][1] < nose_y\n",
    "                            else:\n",
    "                                orientation_data['face_upright'] = False\n",
    "                        else:\n",
    "                            orientation_data['face_upright'] = False\n",
    "                        break\n",
    "            return orientation_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {'has_face': False, 'confidence': 0, 'face_upright': False, 'keypoints': {}}\n",
    "        \n",
    "    def calculate_orientation_score(self, orientation_data):\n",
    "        if not orientation_data['has_face']:\n",
    "            return 0\n",
    "        \n",
    "        score = orientation_data['confidence'] * 100\n",
    "\n",
    "        if orientation_data['face_upright']:\n",
    "            score += 50\n",
    "\n",
    "        keypoint_count = len(orientation_data['keypoints'])\n",
    "        score += keypoint_count * 5\n",
    "\n",
    "        return score\n",
    "    \n",
    "    def rotate_image(self, image, angle):\n",
    "        h, w = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "        cos_val = abs(rotation_matrix[0, 0])\n",
    "        sin_val = abs(rotation_matrix[0, 1])\n",
    "        new_w = int((h * sin_val) + (w * cos_val))\n",
    "        new_h = int((h * cos_val) + (w * sin_val))\n",
    "\n",
    "        rotation_matrix[0, 2] += (new_w / 2) - center[0]\n",
    "        rotation_matrix[1, 2] += (new_h / 2) - center[1]\n",
    "\n",
    "        rotated = cv2.warpAffine(image, rotation_matrix, (new_w, new_h), borderMode=cv2.BORDER_CONSTANT, borderValue=(255, 255, 255))\n",
    "\n",
    "        return rotated\n",
    "    \n",
    "    def extract_crop(self, card, region):\n",
    "        x, y, w, h = region\n",
    "        padding = 5\n",
    "        x = max(0, x - padding)\n",
    "        y = max(0, y - padding)\n",
    "        w = min(card.shape[1] - x, w + 2 * padding)\n",
    "        h = min(card.shape[0] - y, h + 2 * padding)\n",
    "\n",
    "        crop = card[y:y+h, x:x+w]\n",
    "        return crop\n",
    "    \n",
    "    def validate_photo(self, crop):\n",
    "        try:\n",
    "            results = self.detection_model(crop, classes=[0], verbose=False)\n",
    "\n",
    "            for result in results:\n",
    "                if result.boxes is not None and len(result.boxes) > 0:\n",
    "                    max_conf = 0\n",
    "                    if result.boxes.conf is not None and len(result.boxes.conf) > 0:\n",
    "                         max_conf = result.boxes.conf.max().item()\n",
    "                    \n",
    "                    return max_conf > 0.05\n",
    "                \n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "\n",
    "def process_cards(input_dir, output_dir):\n",
    "    processor = CardProcessor()\n",
    "\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    all_images_info = []\n",
    "\n",
    "    for image_file in input_path.glob('*.jpg'):\n",
    "        print(f\"Processing {image_file.name}...\")\n",
    "        results = processor.process_card(str(image_file))\n",
    "\n",
    "        for result in results:\n",
    "            id = result['id']\n",
    "            if id in [\"ID_NOT_FOUND\", \"ID_NOT_DETECTED\", \"OCR_ERROR\"]:\n",
    "                filename = f\"unknown_{result['source_card']}_{result['region_idx']}.jpg\"\n",
    "            else:\n",
    "                filename = f\"{id}.jpg\"\n",
    "\n",
    "            output_file = output_path / filename\n",
    "\n",
    "            counter = 1\n",
    "            while output_file.exists():\n",
    "                if id in [\"ID_NOT_FOUND\", \"ID_NOT_DETECTED\", \"OCR_ERROR\"]:\n",
    "                    filename = f\"unknown_{result['source_card']}_{result['region_idx']}_{counter}.jpg\"\n",
    "                else:\n",
    "                    filename = f\"{id}_{counter}.jpg\"\n",
    "                output_file = output_path / filename\n",
    "                counter += 1\n",
    "            \n",
    "            cv2.imwrite(str(output_file), result['rotated_crop'])\n",
    "\n",
    "            all_images_info.append({\n",
    "                'id': result['id'],\n",
    "                'filename': filename,\n",
    "                'source_card': result['source_card'],\n",
    "                'region_idx': result['region_idx'],\n",
    "                'angle': result['angle'],\n",
    "                'region_x': result['region'][0],\n",
    "                'region_y': result['region'][1],\n",
    "                'region_w': result['region'][2],\n",
    "                'region_h': result['region'][3],\n",
    "            })\n",
    "\n",
    "            print(f\"  Saved: {filename} (ID: {result['id']}, rotated: {result['angle']}°)\")\n",
    "\n",
    "    if all_images_info:\n",
    "        df = pd.DataFrame(all_images_info)\n",
    "        df.to_csv(output_path / 'images_info.csv', index=False)\n",
    "\n",
    "        print(f\"\\nProcessing complete!\")\n",
    "        print(f\"Total images saved: {len(all_images_info)}\")\n",
    "        print(f\"All images saved to: {output_path}\")\n",
    "        print(f\"Image info saved to: {output_path / 'all_images.csv'}\")\n",
    "\n",
    "    return all_images_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10184397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"deit\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 384,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 384,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 64044\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrOCR model loaded successfully.\n",
      "Processing card_2.jpg...\n",
      "YOLO detected and padded 20 raw content regions.\n",
      "Unique content regions after YOLO deduplication: 20\n",
      "Valid content regions after filtering: 20\n",
      "Found 20 image regions.\n",
      "Debug - TrOCR extracted ID: 21 from region (38, 318, 302, 80)\n",
      "Debug - Saved non-numeric OCR crop to debug_ocr_images/ocr_input_1_fails_nonumeric_......png (Raw: '.....')\n",
      "Debug - TrOCR extracted ID: 22 from region (386, 279, 274, 80)\n",
      "Debug - TrOCR extracted ID: 23 from region (702, 276, 282, 80)\n",
      "Debug - TrOCR extracted ID: 25 from region (1366, 270, 298, 80)\n",
      "Debug - TrOCR extracted ID: 30 from region (1365, 686, 304, 80)\n",
      "Debug - Saved non-numeric OCR crop to debug_ocr_images/ocr_input_6_fails_nonumeric_-.png (Raw: '-')\n",
      "Debug - TrOCR extracted ID: 27 from region (374, 640, 286, 80)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_cards\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcards\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextracted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 465\u001b[0m, in \u001b[0;36mprocess_cards\u001b[0;34m(input_dir, output_dir)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m input_path\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 465\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_card\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[64], line 35\u001b[0m, in \u001b[0;36mCardProcessor.process_card\u001b[0;34m(self, card_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m crop\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m crop\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m rotated_crop, angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect_orientation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m is_valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_photo(rotated_crop)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid:\n",
      "Cell \u001b[0;32mIn[64], line 313\u001b[0m, in \u001b[0;36mCardProcessor.correct_orientation\u001b[0;34m(self, crop)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m angle \u001b[38;5;129;01min\u001b[39;00m angles:\n\u001b[1;32m    312\u001b[0m     rotated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotate_image(crop, angle)\n\u001b[0;32m--> 313\u001b[0m     orientation_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_face_orientation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrotated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m orientation_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas_face\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    316\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_orientation_score(orientation_data)\n",
      "Cell \u001b[0;32mIn[64], line 327\u001b[0m, in \u001b[0;36mCardProcessor.analyze_face_orientation\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_face_orientation\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpose_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m         orientation_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas_face\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface_upright\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    333\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m: {},\n\u001b[1;32m    334\u001b[0m         }\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/engine/model.py:185\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    158\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    161\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/engine/model.py:555\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/engine/predictor.py:227\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/engine/predictor.py:330\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 330\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/engine/predictor.py:182\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    178\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    181\u001b[0m )\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:636\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 636\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/nn/tasks.py:138\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/nn/tasks.py:156\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/nn/tasks.py:179\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 179\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    180\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:318\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 318\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:318\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 318\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:353\u001b[0m, in \u001b[0;36mC3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through the CSP bottleneck with 3 convolutions.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:92\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = process_cards('cards', 'extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deccb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('cards/card_2.jpg')\n",
    "#Check region (30, 1012, 245, 120)\n",
    "x, y, w, h = 60, 1040, 245, 65\n",
    "crop = img[y:y+h, x:x+w]\n",
    "plt.imshow(crop)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
