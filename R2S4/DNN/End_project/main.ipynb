{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f758013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import random\n",
    "import pytesseract\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3fe375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3999a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.12).\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "#dataset_path = kagglehub.dataset_download(\"jessicali9530/lfw-dataset\")\n",
    "dataset_path = kagglehub.dataset_download(\"chiragsaipanuganti/morph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f68a3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffdb5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.jpg')):\n",
    "            image_paths.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595db6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_whitecard(image_paths, naming, height=1540, width=1754, im_width=250, im_height=250, spacing=80):\n",
    "    whitecard = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "    x = 10\n",
    "    y = 10\n",
    "\n",
    "    diagonal_size = int(np.sqrt(im_width**2 + im_height**2)) + 10\n",
    "\n",
    "    for i, name in zip(image_paths, naming):\n",
    "        img = cv2.imread(i)\n",
    "        img = cv2.resize(img, (im_width, im_height))\n",
    "        print(i)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        rot_canvas = np.ones((diagonal_size, diagonal_size, 3), dtype=np.uint8) * 255\n",
    "        start_x = (diagonal_size - im_width) // 2\n",
    "        start_y = (diagonal_size - im_height) // 2\n",
    "        rot_canvas[start_y:start_y + im_height, start_x:start_x + im_width] = img\n",
    "\n",
    "        rot_angle = random.uniform(-180, 180)\n",
    "        center = (diagonal_size // 2, diagonal_size // 2)\n",
    "        rot_matrix = cv2.getRotationMatrix2D(center, rot_angle, 1.0)\n",
    "        rot_img = cv2.warpAffine(rot_canvas, rot_matrix, (diagonal_size, diagonal_size), borderValue=(255, 255, 255))\n",
    "\n",
    "        whitecard[y:y+diagonal_size, x:x+diagonal_size] = rot_img\n",
    "\n",
    "        cv2.putText(whitecard, str(name), (x + diagonal_size // 2, y + diagonal_size + (spacing // 2)), \n",
    "                   cv2.FONT_HERSHEY_SCRIPT_COMPLEX, 1.3, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        x += diagonal_size + 40\n",
    "        if x > (width-diagonal_size):\n",
    "            x = 10\n",
    "            y += diagonal_size + spacing\n",
    "        if y > (height-diagonal_size):\n",
    "            return whitecard\n",
    "    return whitecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e2aef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cards(image_paths, num, im_width, im_height):\n",
    "    cards = []\n",
    "    random.shuffle(image_paths)\n",
    "    for i in range(num):\n",
    "        card = populate_whitecard(image_paths[i*20:(i+1)*20], [f\"{i*20+j+1}\" for j in range(20)], im_height=im_height, im_width=im_width)\n",
    "        cards.append(card)\n",
    "    return cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e69643e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/109931_0M32.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/01505_05M28.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/276845_01M20.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/328195_00M16.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/307848_01M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/27366_02F19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/137577_4M34.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/39447_03F24.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/17019_01M29.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/097017_5M35.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/055393_0M43.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/28896_01M16.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/276259_01M20.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/307061_03M18.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/22917_03M17.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/108590_2M37.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/142911_02M34.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/327554_01M39.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/226840_00F23.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/105954_1M38.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/062276_24M47.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/267436_02F46.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/322514_04M48.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/053658_0M48.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/212656_02F28.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/157372_00F31.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/292351_01M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/060771_05M45.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/071121_0M41.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/074580_12M49.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/257500_00M23.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/319928_04M22.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/271176_05M23.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/153563_03F39.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/199162_03M27.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/81329_01M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/263608_01M27.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/132227_06M44.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/69245_05M33.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/294449_00M35.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/334074_00F18.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/072276_3M37.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/051711_2M53.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/035537_01M54.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/327061_01M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/224646_01M23.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/262549_00M38.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/42552_03M23.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/101502_1F39.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/5249_00F19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/083495_1F40.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/62581_03M42.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/239382_00M23.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/071990_3F35.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/249232_00M23.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/320542_04M17.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/117361_0M37.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/067630_01M43.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/071945_4M43.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/051440_1M48.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/269270_03M22.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/27384_10M42.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/240009_06M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/190764_05M27.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/06907_17M36.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/11970_00M26.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/134577_09F35.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/055784_1M45.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/267620_02M23.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/296998_04M20.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/094956_2F41.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/208929_01M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/127971_13M37.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/242552_04M23.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/089052_2M46.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/257699_03M42.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/5810_01M17.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/278177_01M22.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/072421_03F40.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/214404_02F36.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/092198_8M43.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/068872_02M45.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/191920_00M38.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/240845_00F24.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/095947_1M38.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/163308_01M33.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/323438_00F20.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/071990_17F37.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/168941_01M39.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/8813_00M27.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/23257_2M34.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/089898_2M47.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/185053_00M34.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/304061_04M18.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/264304_01M21.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/081549_4M38.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/323319_00M28.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/096223_2M39.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/63120_07M22.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/090071_4M44.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/216536_11M26.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/154746_02M33.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/19200_00M30.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/211160_04M52.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/110142_07M40.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/286189_02M29.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/093513_02M42.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/099488_1M37.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/151982_04M36.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/298559_03F19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/331903_02F26.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/128170_0M31.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/048618_02M55.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/089880_6F43.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/259581_00M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/216565_01M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/290009_00M35.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/094195_3M41.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/134712_04M38.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/28677_04M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/133960_1F45.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/052656_01M49.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/106450_07M41.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/087478_2M36.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/092822_8M36.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/245925_04M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/099857_16F41.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/293556_03M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/151615_00M32.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/240166_06M24.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/138789_09M33.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/113372_1M38.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/133235_02M33.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/5375_01M20.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/318845_02F33.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/61316_03M24.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/275630_03M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/333580_03M17.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/305090_02M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/01322_03M26.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/341941_01M23.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/218020_07F39.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/066648_1M51.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/216656_03M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/066291_08M42.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/252709_04M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/164943_02M37.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/209240_02M25.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/312111_02M31.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/47872_00M29.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/94658_01M34.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/206860_00M28.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/211343_01M29.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/281763_01F24.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/287443_03M22.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/094818_04F49.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/078572_4M42.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/268100_01M18.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/075391_8M47.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/168741_02M48.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/337662_01M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/303914_01M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/305192_00M18.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/335048_00M17.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/157609_04M45.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/337165_00M17.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/074097_1M49.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/283608_01M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/104333_02F43.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/258854_00M33.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/084785_2F48.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/116815_03M37.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/03832_01M27.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/23414_04M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/240422_04M35.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/70255_03M30.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/08616_00M17.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/075932_0M36.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/304771_03M24.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/45831_00M22.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/328454_03F21.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/315184_03M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/340006_01M16.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/226745_03M26.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/072594_1M38.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/72059_01F20.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/262858_01M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/304998_01M18.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/109987_0M36.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/117513_5M34.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/049708_2M49.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/08486_2M35.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/324490_00M29.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/324217_03M19.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/287240_01M21.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/275595_12M43.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/284566_00M20.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Validation/312953_02M36.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Train/227875_03M24.JPG\n",
      "/root/.cache/kagglehub/datasets/chiragsaipanuganti/morph/versions/2/Dataset/Images/Test/104365_2M34.JPG\n"
     ]
    }
   ],
   "source": [
    "cards = get_cards(image_paths, 10, im_width=200, im_height=200)\n",
    "\n",
    "output_path = Path(\"cards\")\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "for i, card in enumerate(cards):\n",
    "    card_bgr = cv2.cvtColor(card, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(f'cards/card_{i+1}.jpg', card_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9949717",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11n.pt')  # nano\n",
    "model = YOLO('yolo11s.pt')  # small\n",
    "model = YOLO('yolo11m.pt')  # medium\n",
    "model = YOLO('yolo11l.pt')  # large\n",
    "model = YOLO('yolo11x.pt')  # extra large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "325a5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11n-pose.pt')  # nano\n",
    "model = YOLO('yolo11s-pose.pt')  # small\n",
    "model = YOLO('yolo11m-pose.pt')  # medium\n",
    "model = YOLO('yolo11l-pose.pt')  # large\n",
    "model = YOLO('yolo11x-pose.pt')  # extra large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2660841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardProcessor:\n",
    "    def __init__(self, detection_model=\"yolo11x.pt\", pose_model=\"yolo11x-pose.pt\"):\n",
    "        self.detection_model = YOLO(detection_model)\n",
    "        self.pose_model = YOLO(pose_model)\n",
    "    \n",
    "    def process_card(self, card_path):\n",
    "        card = cv2.imread(card_path)\n",
    "\n",
    "        image_regions = self.detect_image_regions(card)\n",
    "\n",
    "        print(f\"Found {len(image_regions)} image regions.\")\n",
    "\n",
    "        results = []\n",
    "        valid_count = 0\n",
    "        for i, region in enumerate(image_regions):\n",
    "            crop = self.extract_crop(card, region)\n",
    "            if crop.shape[0] < 50 or crop.shape[1] < 50:\n",
    "                continue\n",
    "        \n",
    "            rotated_crop, angle = self.correct_orientation(crop)\n",
    "        \n",
    "            is_valid = self.validate_photo(rotated_crop)\n",
    "\n",
    "            if is_valid:\n",
    "                valid_count += 1\n",
    "                id = self.find_id(card, region)\n",
    "\n",
    "                results.append({\n",
    "                    'id': id,\n",
    "                    'rotated_crop': rotated_crop,\n",
    "                    'angle': angle,\n",
    "                    'source_card': Path(card_path).name,\n",
    "                    'region_idx': i,\n",
    "                    'region': region\n",
    "                })\n",
    "        print(f\"Valid faces found: {valid_count}/{len(image_regions)}\")\n",
    "        return results\n",
    "    \n",
    "    def detect_image_regions(self, card):\n",
    "        #gray = cv2.cvtColor(card, cv2.COLOR_BGR2GRAY)\n",
    "#\n",
    "        #regions = []\n",
    "#\n",
    "        #edges1 = cv2.Canny(gray, 30, 100, apertureSize=3)\n",
    "        #contours1, _ = cv2.findContours(edges1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        #regions.extend(self.extract_regions(contours1, card.shape))\n",
    "#\n",
    "        #print(len(regions), \"regions found after Canny edge detection.\")\n",
    "#\n",
    "        #adaptive2 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "        #edges2 = cv2.Canny(adaptive2, 50, 150)\n",
    "        #contours2, _ = cv2.findContours(edges2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        #regions.extend(self.extract_regions(contours2, card.shape))\n",
    "#\n",
    "        #print(len(regions), \"regions found after adaptive thresholding and Canny edge detection.\")\n",
    "#\n",
    "        #kernel3 = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n",
    "        #morph3 = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel3)\n",
    "        #edges3 = cv2.Canny(morph3, 50, 150)\n",
    "        #contours3, _ = cv2.findContours(edges3, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        #regions.extend(self.extract_regions(contours3, card.shape))\n",
    "#\n",
    "        #print(len(regions), \"regions found after morphological operations and Canny edge detection.\")\n",
    "\n",
    "        results = self.detection_model(card, classes=[0], verbose=False)\n",
    "\n",
    "        regions = []\n",
    "\n",
    "        for result in results:\n",
    "            if result.boxes is not None:\n",
    "                for box in result.boxes:\n",
    "                    if box.conf[0] > 0.35:\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        x, y, w, h = int(x1), int(y1), int(x2 - x1), int(y2 - y1)\n",
    "                        regions.append((x, y, w, h))\n",
    "\n",
    "        print(f\"Regions detected by YOLO: {len(regions)}\")  \n",
    "\n",
    "        print(f\"Total regions before deduplication: {len(regions)}\")\n",
    "        unique_regions = self.remove_duplicate_regions(regions)\n",
    "        print(f\"Unique regions after deduplication: {len(unique_regions)}\")\n",
    "        #valid_regions = self.filter_valid_regions(unique_regions, card.shape)\n",
    "        #print(f\"Valid regions after filtering: {len(valid_regions)}\")\n",
    "\n",
    "        final_regions = sorted(unique_regions, key=lambda r: (r[1], r[0]))\n",
    "\n",
    "        return final_regions\n",
    "    \n",
    "    def extract_regions(self, contours, card_shape):\n",
    "        regions = []\n",
    "        card_area = card_shape[0] * card_shape[1]\n",
    "\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "\n",
    "            if area < card_area * 0.002 or area > card_area * 0.5:\n",
    "                continue\n",
    "\n",
    "            epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "            if len(approx) >= 4:\n",
    "                x, y, w, h = cv2.boundingRect(approx)\n",
    "\n",
    "                ratio = w / h if h > 0 else 0\n",
    "\n",
    "                if 0.3 < ratio < 3.0:\n",
    "                    regions.append((x, y, w, h))\n",
    "\n",
    "        return regions\n",
    "    \n",
    "    def remove_duplicate_regions(self, regions):\n",
    "        if not regions:\n",
    "            return []\n",
    "        \n",
    "        unique_regions = []\n",
    "\n",
    "        for region in regions:\n",
    "            is_duplicate = False\n",
    "            x1, y1, w1, h1 = region\n",
    "\n",
    "            for registered in unique_regions:\n",
    "                x2, y2, w2, h2 = registered\n",
    "\n",
    "                iou = self.calculate_iou((x1, y1, w1, h1), (x2, y2, w2, h2))\n",
    "\n",
    "                if iou > 0.7:\n",
    "                    is_duplicate = True\n",
    "                    break\n",
    "\n",
    "            if not is_duplicate:\n",
    "                unique_regions.append(region)\n",
    "\n",
    "        return unique_regions\n",
    "    \n",
    "    def calculate_iou(self, region1, region2):\n",
    "        x1, y1, w1, h1 = region1\n",
    "        x2, y2, w2, h2 = region2\n",
    "\n",
    "        x_intersect = max(x1, x2)\n",
    "        y_intersect = max(y1, y2)\n",
    "        w_intersect = min(x1 + w1, x2 + w2)\n",
    "        h_intersect = min(y1 + h1, y2 + h2)\n",
    "\n",
    "        if w_intersect <= x_intersect or h_intersect <= y_intersect:\n",
    "            return 0\n",
    "        \n",
    "        intersection = (w_intersect - x_intersect) * (h_intersect - y_intersect)\n",
    "\n",
    "        area1 = w1 * h1\n",
    "        area2 = w2 * h2\n",
    "        union = area1 + area2 - intersection\n",
    "\n",
    "        return intersection / union if union > 0 else 0\n",
    "    \n",
    "    def filter_valid_regions(self, regions, card_shape):\n",
    "        valid_regions = []\n",
    "        card_h, card_w = card_shape[:2]\n",
    "        min_size = 100\n",
    "        max_size = min(card_h, card_w) // 3\n",
    "\n",
    "        for x, y, w, h in regions:\n",
    "            aspect_ratio = max(w, h) / min(w, h)\n",
    "            if aspect_ratio > 2.0:\n",
    "                continue\n",
    "            if min_size <= min(w, h) and max(w, h) <= max_size:\n",
    "                margin = 5\n",
    "                if (margin < x < card_w - w - margin and\n",
    "                    margin < y < card_h - h - margin):\n",
    "                    valid_regions.append((x, y, w, h))\n",
    "\n",
    "        return valid_regions\n",
    "    \n",
    "    def find_id(self, card, region):\n",
    "        x, y, w, h = region\n",
    "\n",
    "        search_areas = []\n",
    "\n",
    "        below_y = y + h + 10\n",
    "        below_h = min(120, card.shape[0] - below_y)\n",
    "\n",
    "        if below_y + below_h <= card.shape[0]:\n",
    "            search_x = max(0, x - 20)\n",
    "            search_w = min(card.shape[1] - search_x, w + 40)\n",
    "            search_areas.append(('below', (search_x, below_y, search_w, below_h)))\n",
    "        \n",
    "        #above_h = min(60, y)\n",
    "        #if above_h > 10:\n",
    "        #    search_areas.append(('above', (x, max(0, y - above_h), w, above_h)))\n",
    "\n",
    "        #right_x = x + w + 2\n",
    "        #right_w = min(100, card.shape[1] - right_x)\n",
    "        #if right_x + right_w <= card.shape[1]:\n",
    "        #    search_areas.append(('right', (right_x, y, right_w, h)))\n",
    "        \n",
    "        #left_w = min(100, x)\n",
    "        #if left_w > 10:\n",
    "        #    search_areas.append(('left', (max(0, x - left_w), y, left_w, h)))\n",
    "\n",
    "        for location, (sx, sy, sw, sh) in search_areas:\n",
    "            id = self.extract_id(card, (sx, sy, sw, sh))\n",
    "\n",
    "            if id not in [\"ID_NOT_FOUND\", \"ID_NOT_DETECTED\", \"OCR_ERROR\"]:\n",
    "                return id\n",
    "            \n",
    "        return \"ID_NOT_DETECTED\"\n",
    "    \n",
    "    def extract_id(self, card, region):\n",
    "        x, y, w, h = region\n",
    "\n",
    "        x = max(0, x)\n",
    "        y = max(0, y)\n",
    "        w = min(w, card.shape[1] - x)\n",
    "        h = min(h, card.shape[0] - y)\n",
    "\n",
    "        if w <= 0 or h <= 0:\n",
    "            print(f\"Debug - Invalid crop dimensions for OCR: w={w}, h={h}\")\n",
    "            return \"ID_NOT_FOUND\"\n",
    "        \n",
    "        id_crop = card[y:y+h, x:x+w]\n",
    "\n",
    "        try:\n",
    "            gray = cv2.cvtColor(id_crop, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            all_confident_digits = []\n",
    "\n",
    "            preproc_methods = {\n",
    "                'simple_binary': lambda g: cv2.threshold(g, 127, 255, cv2.THRESH_BINARY)[1],\n",
    "                'otsu': lambda g: cv2.threshold(g, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1],\n",
    "                'adaptive_gaussian': lambda g: cv2.adaptiveThreshold(g, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 5)\n",
    "            }\n",
    "\n",
    "            tesseract_configs = [\n",
    "                r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789',\n",
    "                r'--oem 3 --psm 7 -c tessedit_char_whitelist=0123456789',\n",
    "                r'--oem 3 --psm 8 -c tessedit_char_whitelist=0123456789',\n",
    "                r'--oem 3 --psm 13 -c tessedit_char_whitelist=0123456789'\n",
    "            ]\n",
    "            \n",
    "            min_crop_dim_for_ocr = 10\n",
    "            if gray.shape[0] < min_crop_dim_for_ocr or gray.shape[1] < min_crop_dim_for_ocr:\n",
    "                print(f\"Debug - OCR crop too small: {gray.shape}\")\n",
    "                return \"ID_NOT_DETECTED\"\n",
    "\n",
    "            for method_name, preproc_func in preproc_methods.items():\n",
    "                try:\n",
    "                    thresh_img = preproc_func(gray.copy())\n",
    "                except cv2.error as e:\n",
    "                    print(f\"Debug - OpenCV error in {method_name} preprocessing: {e}\")\n",
    "                    continue\n",
    "\n",
    "                for scale_factor in [2, 3]:\n",
    "                    scaled_h, scaled_w = thresh_img.shape[0] * scale_factor, thresh_img.shape[1] * scale_factor\n",
    "                    if scaled_h == 0 or scaled_w == 0:\n",
    "                        continue\n",
    "                    scaled_img = cv2.resize(thresh_img, (scaled_w, scaled_h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                    for config_str in tesseract_configs:\n",
    "                        try:\n",
    "                            data = pytesseract.image_to_data(scaled_img, config=config_str, output_type=pytesseract.Output.DICT)\n",
    "                            page_text_segments = []\n",
    "                            for i in range(len(data['text'])):\n",
    "                                text_segment = data['text'][i].strip()\n",
    "                                confidence = int(data['conf'][i])\n",
    "                                if confidence > 50 and text_segment:\n",
    "                                    digits_in_segment = ''.join(filter(str.isdigit, text_segment))\n",
    "                                    if digits_in_segment:\n",
    "                                        page_text_segments.append(digits_in_segment)\n",
    "                            if page_text_segments:\n",
    "                                combined_digits = \"\".join(page_text_segments)\n",
    "                                if combined_digits:\n",
    "                                     all_confident_digits.append(combined_digits)\n",
    "                        except RuntimeError:\n",
    "                            pass\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "            if all_confident_digits:\n",
    "                from collections import Counter\n",
    "                \n",
    "                if len(all_confident_digits) > 5:\n",
    "                    lengths = [len(s) for s in all_confident_digits]\n",
    "                    median_len = sorted(lengths)[len(lengths)//2]\n",
    "                    reasonable_results = [r for r in all_confident_digits if abs(len(r) - median_len) <= 1 or len(r) == median_len]\n",
    "                    if not reasonable_results:\n",
    "                        reasonable_results = all_confident_digits\n",
    "                else:\n",
    "                    reasonable_results = all_confident_digits\n",
    "\n",
    "                if not reasonable_results:\n",
    "                     print(f\"Debug - No reasonable_results from all_confident_digits: {all_confident_digits}\")\n",
    "                     return \"ID_NOT_DETECTED\"\n",
    "\n",
    "                counts = Counter(reasonable_results)\n",
    "                sorted_by_freq = counts.most_common()\n",
    "\n",
    "                if not sorted_by_freq:\n",
    "                    print(f\"Debug - No results after Counter on reasonable_results: {reasonable_results}\")\n",
    "                    return \"ID_NOT_DETECTED\"\n",
    "\n",
    "                chosen_id = sorted_by_freq[0][0] \n",
    "\n",
    "                if len(sorted_by_freq) > 1:\n",
    "                    val1, c1 = sorted_by_freq[0]\n",
    "                    val2, c2 = sorted_by_freq[1]\n",
    "                    \n",
    "                    # Apply suffix heuristic if counts are close (c1 is not much larger than c2)\n",
    "                    if c1 <= c2 * 1.5 : # e.g., 8 vs 6 is true (8 <= 9), 10 vs 6 is false (10 <= 9)\n",
    "                        # Check if val2 (second most common) is shorter by one char and a suffix of val1 (most common)\n",
    "                        if len(val1) == len(val2) + 1 and val1.endswith(val2):\n",
    "                            chosen_id = val2\n",
    "                            print(f\"Debug - Suffix Heuristic Applied: Chose '{val2}' (count {c2}) over '{val1}' (count {c1})\")\n",
    "                        # Check if val1 (most common) is shorter by one char and a suffix of val2 (second most common)\n",
    "                        # This case is less likely if val1 is already the most_common, but good for completeness\n",
    "                        elif len(val2) == len(val1) + 1 and val2.endswith(val1):\n",
    "                            # chosen_id is already val1, so no change, but we can log it\n",
    "                            print(f\"Debug - Suffix Heuristic Confirmed: '{val1}' (count {c1}) is shorter suffix of '{val2}' (count {c2})\")\n",
    "                \n",
    "                print(f\"Debug - All confident digits: {all_confident_digits}, Filtered (reasonable): {reasonable_results}, Chosen: {chosen_id}\")\n",
    "                return chosen_id\n",
    "            else:\n",
    "                print(f\"Debug - No confident digits found after all attempts.\")\n",
    "                return \"ID_NOT_DETECTED\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Debug - OCR_ERROR exception in extract_id: {e}\")\n",
    "            return \"OCR_ERROR\"\n",
    "        \n",
    "    def correct_orientation(self, crop):\n",
    "        best_crop = crop.copy()\n",
    "        best_angle = 0\n",
    "        best_score = 0\n",
    "\n",
    "        angles = list(range(0, 360, 15))\n",
    "\n",
    "        for angle in angles:\n",
    "            if angle == 0:\n",
    "                rotated = crop\n",
    "            else:\n",
    "                rotated = self.rotate_image(crop, angle)\n",
    "\n",
    "            orientation_data = self.analyze_face_orientation(rotated)\n",
    "\n",
    "            if orientation_data['has_face']:\n",
    "                score = self.calculate_orientation_score(orientation_data)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_crop = rotated.copy()\n",
    "                    best_angle = angle\n",
    "\n",
    "        if best_angle > 0:\n",
    "            angles = list(range(best_angle - 10, best_angle + 11, 1))\n",
    "\n",
    "            for angle in angles:\n",
    "                rotated = self.rotate_image(crop, angle)\n",
    "                orientation_data = self.analyze_face_orientation(rotated)\n",
    "\n",
    "                if orientation_data['has_face']:\n",
    "                    score = self.calculate_orientation_score(orientation_data)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_crop = rotated.copy()\n",
    "                        best_angle = angle\n",
    "\n",
    "        return best_crop, best_angle\n",
    "\n",
    "    def analyze_face_orientation(self, image):\n",
    "\n",
    "        try:\n",
    "            results = self.pose_model(image, verbose=False)\n",
    "\n",
    "            orientation_data = {\n",
    "                'has_face': False,\n",
    "                'confidence': 0,\n",
    "                'face_upright': False,\n",
    "                'keypoints': {},\n",
    "            }\n",
    "\n",
    "            for result in results:\n",
    "                keypoints = result.keypoints\n",
    "                boxes = result.boxes\n",
    "\n",
    "                if keypoints is not None and boxes is not None and len(boxes) > 0:\n",
    "                    best_idx = torch.argmax(boxes.conf).item()\n",
    "                    kp = keypoints[best_idx]\n",
    "                    box_conf = boxes.conf[best_idx].item()\n",
    "\n",
    "                    nose = kp.xy[0][0]\n",
    "                    left_eye = kp.xy[0][1]\n",
    "                    right_eye = kp.xy[0][2]\n",
    "\n",
    "                    valid_keypoints = 0\n",
    "                    if len(nose) == 2 and nose[0] > 0 and nose[1] > 0:\n",
    "                        orientation_data['keypoints']['nose'] = nose.tolist()\n",
    "                        valid_keypoints += 1\n",
    "\n",
    "                    if len(left_eye) == 2 and left_eye[0] > 0 and left_eye[1] > 0:\n",
    "                        orientation_data['keypoints']['left_eye'] = left_eye.tolist()\n",
    "                        valid_keypoints += 1\n",
    "\n",
    "                    if len(right_eye) == 2 and right_eye[0] > 0 and right_eye[1] > 0:\n",
    "                        orientation_data['keypoints']['right_eye'] = right_eye.tolist()\n",
    "                        valid_keypoints += 1\n",
    "\n",
    "                    if valid_keypoints >= 2:\n",
    "                        orientation_data['has_face'] = True\n",
    "                        orientation_data['confidence'] = box_conf\n",
    "\n",
    "                        if 'nose' in orientation_data['keypoints']:\n",
    "                            nose_y = orientation_data['keypoints']['nose'][1]\n",
    "                            eye_above = True\n",
    "\n",
    "                            for eye in ['left_eye', 'right_eye']:\n",
    "                                if eye in orientation_data['keypoints']:\n",
    "                                    if orientation_data['keypoints'][eye][1] >= nose_y:\n",
    "                                        eye_above = False\n",
    "\n",
    "                            orientation_data['face_upright'] = eye_above\n",
    "\n",
    "                        break\n",
    "            return orientation_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {'has_face': False, 'confidence': 0, 'face_upright': False, 'keypoints': {}}\n",
    "        \n",
    "    def calculate_orientation_score(self, orientation_data):\n",
    "        if not orientation_data['has_face']:\n",
    "            return 0\n",
    "        \n",
    "        score = orientation_data['confidence'] * 100\n",
    "\n",
    "        if orientation_data['face_upright']:\n",
    "            score += 50\n",
    "\n",
    "        keypoint_count = len(orientation_data['keypoints'])\n",
    "        score += keypoint_count * 5\n",
    "\n",
    "        return score\n",
    "    \n",
    "    def rotate_image(self, image, angle):\n",
    "        h, w = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "        cos_val = abs(rotation_matrix[0, 0])\n",
    "        sin_val = abs(rotation_matrix[0, 1])\n",
    "        new_w = int((h * sin_val) + (w * cos_val))\n",
    "        new_h = int((h * cos_val) + (w * sin_val))\n",
    "\n",
    "        rotation_matrix[0, 2] += (new_w / 2) - center[0]\n",
    "        rotation_matrix[1, 2] += (new_h / 2) - center[1]\n",
    "\n",
    "        rotated = cv2.warpAffine(image, rotation_matrix, (new_w, new_h), borderMode=cv2.BORDER_CONSTANT, borderValue=(255, 255, 255))\n",
    "\n",
    "        return rotated\n",
    "    \n",
    "    def extract_crop(self, card, region):\n",
    "        x, y, w, h = region\n",
    "        padding = 5\n",
    "        x = max(0, x - padding)\n",
    "        y = max(0, y - padding)\n",
    "        w = min(card.shape[1] - x, w + 2 * padding)\n",
    "        h = min(card.shape[0] - y, h + 2 * padding)\n",
    "\n",
    "        crop = card[y:y+h, x:x+w]\n",
    "        return crop\n",
    "    \n",
    "    def validate_photo(self, crop):\n",
    "        try:\n",
    "            results = self.detection_model(crop, classes=[0], verbose=False)\n",
    "\n",
    "            for result in results:\n",
    "                if result.boxes is not None and len(result.boxes) > 0:\n",
    "                    max_conf = max([box.conf[0].item() for box in result.boxes])\n",
    "                    return max_conf > 0.1\n",
    "                \n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "\n",
    "def process_cards(input_dir, output_dir):\n",
    "    processor = CardProcessor()\n",
    "\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    all_images_info = []\n",
    "\n",
    "    for image_file in input_path.glob('*.jpg'):\n",
    "        print(f\"Processing {image_file.name}...\")\n",
    "        results = processor.process_card(str(image_file))\n",
    "\n",
    "        for result in results:\n",
    "            id = result['id']\n",
    "            if id in [\"ID_NOT_FOUND\", \"ID_NOT_DETECTED\", \"OCR_ERROR\"]:\n",
    "                filename = f\"unknown_{result['source_card']}_{result['region_idx']}.jpg\"\n",
    "            else:\n",
    "                filename = f\"{id}.jpg\"\n",
    "\n",
    "            output_file = output_path / filename\n",
    "\n",
    "            counter = 1\n",
    "            while output_file.exists():\n",
    "                if id in [\"ID_NOT_FOUND\", \"ID_NOT_DETECTED\", \"OCR_ERROR\"]:\n",
    "                    filename = f\"unknown_{result['source_card']}_{result['region_idx']}_{counter}.jpg\"\n",
    "                else:\n",
    "                    filename = f\"{id}_{counter}.jpg\"\n",
    "                output_file = output_path / filename\n",
    "                counter += 1\n",
    "            \n",
    "            cv2.imwrite(str(output_file), result['rotated_crop'])\n",
    "\n",
    "            all_images_info.append({\n",
    "                'id': result['id'],\n",
    "                'filename': filename,\n",
    "                'source_card': result['source_card'],\n",
    "                'region_idx': result['region_idx'],\n",
    "                'angle': result['angle'],\n",
    "                'region_x': result['region'][0],\n",
    "                'region_y': result['region'][1],\n",
    "                'region_w': result['region'][2],\n",
    "                'region_h': result['region'][3],\n",
    "            })\n",
    "\n",
    "            print(f\"  Saved: {filename} (ID: {result['id']}, rotated: {result['angle']}°)\")\n",
    "\n",
    "    if all_images_info:\n",
    "        df = pd.DataFrame(all_images_info)\n",
    "        df.to_csv(output_path / 'images_info.csv', index=False)\n",
    "\n",
    "        print(f\"\\nProcessing complete!\")\n",
    "        print(f\"Total images saved: {len(all_images_info)}\")\n",
    "        print(f\"All images saved to: {output_path}\")\n",
    "        print(f\"Image info saved to: {output_path / 'all_images.csv'}\")\n",
    "\n",
    "    return all_images_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10184397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing card_2.jpg...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_cards\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcards\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextracted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 493\u001b[0m, in \u001b[0;36mprocess_cards\u001b[0;34m(input_dir, output_dir)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m input_path\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 493\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_card\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m, in \u001b[0;36mCardProcessor.process_card\u001b[0;34m(self, card_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_card\u001b[39m(\u001b[38;5;28mself\u001b[39m, card_path):\n\u001b[1;32m      7\u001b[0m     card \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(card_path)\n\u001b[0;32m----> 9\u001b[0m     image_regions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_image_regions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(image_regions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m image regions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[21], line 65\u001b[0m, in \u001b[0;36mCardProcessor.detect_image_regions\u001b[0;34m(self, card)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetect_image_regions\u001b[39m(\u001b[38;5;28mself\u001b[39m, card):\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m#gray = cv2.cvtColor(card, cv2.COLOR_BGR2GRAY)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m#print(len(regions), \"regions found after morphological operations and Canny edge detection.\")\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetection_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m         regions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/ultralytics/engine/model.py:185\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    158\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    161\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/ultralytics/engine/model.py:548\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/ultralytics/engine/predictor.py:385\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[0;34m(self, model, verbose)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m    Initialize YOLO model with given parameters and set it to evaluation mode.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m        verbose (bool): Whether to print verbose output.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:201\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[1;32m    200\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mfuse(verbose\u001b[38;5;241m=\u001b[39mverbose)  \u001b[38;5;66;03m# fuse before move to gpu\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkpt_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    203\u001b[0m     kpt_shape \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/ultralytics/nn/tasks.py:289\u001b[0m, in \u001b[0;36mBaseModel._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    Apply a function to all tensors in the model that are not parameters or registered buffers.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m        (BaseModel): An updated BaseModel object.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Detect()\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    292\u001b[0m         m, Detect\n\u001b[1;32m    293\u001b[0m     ):  \u001b[38;5;66;03m# includes all Detect subclasses like Segment, Pose, OBB, WorldDetect, YOLOEDetect, YOLOESegment\u001b[39;00m\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/SI/si/lib/python3.11/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "results = process_cards('cards', 'extracted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
