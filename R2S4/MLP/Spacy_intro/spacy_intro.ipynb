{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/SI/si/lib/python3.11/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from IPython.display import display\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "tree kangaroos\n",
      "tree kangaroos and narwhals\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "first_token = doc[0]\n",
    "print(first_token.text)\n",
    "\n",
    "tree_kangaroos = doc[2:4]\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "tree_kangaroos_and_narwhals = doc[2:6]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        next_token = doc[token.i + 1]\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It          PRON      nsubj     \n",
      "'s          AUX       ccomp     \n",
      "official    ADJ       acomp     \n",
      ":           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "is          AUX       ROOT      \n",
      "the         DET       det       \n",
      "first       ADJ       amod      \n",
      "U.S.        PROPN     nmod      \n",
      "public      ADJ       amod      \n",
      "company     NOUN      attr      \n",
      "to          PART      aux       \n",
      "reach       VERB      relcl     \n",
      "a           DET       det       \n",
      "$           SYM       quantmod  \n",
      "1           NUM       compound  \n",
      "trillion    NUM       nummod    \n",
      "market      NOUN      compound  \n",
      "value       NOUN      dobj      \n",
      "\n",
      "\n",
      "Apple ORG\n",
      "first ORDINAL\n",
      "U.S. GPE\n",
      "$1 trillion MONEY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7-8\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"It's official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Missing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "iphone_x = doc[1:3]\n",
    "print(\"Missing entity:\", iphone_x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n",
      "\n",
      "2018 FIFA World Cup:\n",
      "\n",
      "loved dogs\n",
      "love cats\n",
      "\n",
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "pattarn = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattarn])\n",
    "\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "pattern = [{\"IS_DIGIT\": True}, {\"LOWER\": \"fifa\"}, {\"LOWER\": \"world\"}, {\"LOWER\": \"cup\"}, {\"IS_PUNCT\": True}]\n",
    "matcher.add(\"FIFA_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"LV_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "pattern = [{\"LEMMA\": \"buy\"}, {\"POS\": \"DET\", \"OP\": \"?\"}, {\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"BUY_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked as Apple reveals pre-orders\")\n",
    "\n",
    "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n",
      "\n",
      "Part 2\n",
      "\n",
      "Total matches found: 3\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Match found: download Winzip\n",
      "\n",
      "Part 3\n",
      "\n",
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    ")\n",
    "\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)\n",
    "\n",
    "print(\"\\nPart 2\\n\")\n",
    "\n",
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
    "spacy.explain(\"PROPN\")\n",
    "\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)\n",
    "\n",
    "print(\"\\nPart 3\\n\")\n",
    "\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"*\"}]\n",
    "\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coffee'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "#Chapter 2\n",
    "#1\n",
    "nlp.vocab.strings.add(\"coffee\")\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "display(nlp.vocab.strings[3197928453018144401])\n",
    "nlp.vocab.strings[\"coffee\"]\n",
    "\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5439657043933447811"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PERSON'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "display(cat_hash)\n",
    "display(nlp.vocab.strings[cat_hash])\n",
    "\n",
    "print(\"\\nPart 2\\n\")\n",
    "\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "display(person_hash)\n",
    "nlp.vocab.strings[person_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Hello world,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#4\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "doc = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "display(doc)\n",
    "\n",
    "span = spacy.tokens.Span(doc, 0, 2)\n",
    "\n",
    "span_with_label = spacy.tokens.Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "doc.ents = [span_with_label]\n",
    "\n",
    "display(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n",
      "\n",
      "Part 2\n",
      "\n",
      "Go, get started!\n",
      "\n",
      "Part 3\n",
      "\n",
      "Oh, really?\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "doc = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "print(\"\\nPart 2\\n\")\n",
    "\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "doc = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "print(\"\\nPart 3\\n\")\n",
    "\n",
    "words = [\"Oh\", \",\", \"really\", \"?\"]\n",
    "spaces = [False, True, False, False]\n",
    "\n",
    "doc = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "doc = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "span = spacy.tokens.Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "doc.ents = [span]\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8382381200790405\n",
      "1.0\n",
      "0.2274085134267807\n",
      "0.5528545379638672\n",
      "[-9.9847e-01  1.5255e-01  9.0905e-01  5.0226e-01  3.9668e-01  1.1185e+00\n",
      " -6.8643e-02 -2.0645e-01  1.6348e-01  1.4820e+00 -3.7295e-01  7.4472e-01\n",
      "  1.3312e-01 -3.9140e-01  3.8056e-01 -6.2452e-01  5.6385e-01  9.2489e-01\n",
      "  1.5690e-02  4.6075e-01 -2.4564e-01  4.2107e-01  4.3966e-02 -9.3159e-02\n",
      " -3.7071e-01 -4.8146e-01 -6.4922e-01  3.9712e-01 -2.0001e-01 -7.6328e-01\n",
      " -1.5941e-01  6.8756e-01  7.0181e-01 -7.5998e-01  2.7529e-01  4.6337e-02\n",
      " -1.5418e-02  1.8180e-01  4.5510e-01  1.0663e+00 -2.9151e-01 -2.1489e-01\n",
      "  1.2203e-01  4.2934e-02  8.3894e-02  4.4703e-01 -1.0444e-01  1.1424e-01\n",
      " -4.7985e-01  5.6945e-01 -1.6874e-01  2.2438e-01  7.0262e-02 -3.7424e-01\n",
      "  3.7648e-01  1.1642e-01  1.9766e-01  3.0820e-01 -2.2964e-01  2.1171e-01\n",
      "  3.1852e-01 -1.0459e-02  4.6802e-01  3.5646e-01  2.2602e-01 -7.3830e-01\n",
      "  3.4500e-01 -1.2327e-01 -1.5291e-01  3.0218e-01  5.3695e-02  7.8704e-01\n",
      "  6.0712e-02  4.7265e-01 -7.4620e-01 -1.9323e-01  3.0870e-01  3.6358e-01\n",
      " -3.7764e-02 -6.2496e-01 -3.1335e-01 -9.6310e-02  1.1919e-01 -3.7802e-01\n",
      "  8.2767e-02 -3.2793e-02  9.6121e-01  5.7773e-01 -1.4837e-01 -2.6352e-01\n",
      " -2.3725e-01 -5.3231e-01 -1.1165e-03 -1.4867e-01 -3.9044e-01  5.3478e-01\n",
      " -8.3532e-01  3.9416e-01 -2.7202e-01 -4.1502e-02  7.1965e-02  6.8376e-02\n",
      "  2.2626e-01  4.1723e-01  3.6467e-02 -1.0274e+00  5.9080e-01  5.4467e-02\n",
      "  7.5080e-01 -3.5328e-01  2.8671e-02 -6.9481e-01 -2.1662e-01 -6.2072e-01\n",
      "  3.0580e-01  2.7423e-01  3.5253e-01 -2.8479e-01  2.3088e-01 -8.7428e-02\n",
      "  1.1745e-01  5.8940e-01  2.0204e-02 -6.3295e-01  2.1334e-01 -1.3089e-01\n",
      "  3.5053e-01 -6.0513e-01  6.0901e-02 -2.9444e-01 -3.0708e-02 -1.8844e-01\n",
      "  6.6813e-01  9.0951e-02 -1.6296e-01 -2.6184e-01  1.5688e-01 -4.9181e-01\n",
      "  1.5685e-01 -5.6426e-01 -1.9218e+00 -3.5015e-01  5.1144e-02  4.8787e-01\n",
      " -2.1842e-01 -1.2162e+00 -6.7083e-01  7.5633e-01  5.5967e-01  2.8570e-01\n",
      "  3.8317e-02  2.0527e-01  6.3049e-01 -1.2270e-01  1.7356e-01 -2.0788e-02\n",
      "  4.7214e-02  3.8865e-02  4.8109e-01  2.8895e-01 -6.0584e-01  5.7282e-01\n",
      " -5.5181e-01 -1.0028e-01 -7.6442e-01  1.9320e-01  2.3522e-01 -5.3289e-01\n",
      "  1.2428e-01 -4.0413e-01  6.6727e-01  4.6886e-01 -1.7201e-01 -5.0222e-01\n",
      " -5.4365e-01 -8.7743e-02  6.1381e-02  2.3367e-01  3.5205e-01  2.4652e-01\n",
      "  9.1290e-03 -1.4776e-01 -1.9608e-02  1.3913e-01 -5.2631e-01  1.9159e-01\n",
      "  1.6640e-01 -4.2506e-01  1.0703e-02 -3.8851e-01 -6.5475e-03  3.8810e-01\n",
      " -2.0797e-01 -1.6430e-01 -1.4385e-01  1.6739e-02  6.3590e-01  1.6341e-01\n",
      "  2.5036e-01 -3.1547e-01 -3.1922e-02 -1.7217e-01  1.5988e-01 -4.1263e-02\n",
      "  8.5189e-01 -1.1921e-01 -1.2520e-01  2.9389e-01  8.1719e-01  5.4587e-01\n",
      " -3.1059e-01 -8.8621e-02 -9.9322e-02  2.0033e-01 -3.6203e-01 -1.2742e-01\n",
      " -2.2143e-01  1.7671e-01 -3.6141e-01  5.4365e-01 -5.1876e-01  2.6792e-01\n",
      " -3.4876e-01  2.3043e-01  4.2106e-01  3.6772e-01 -1.4128e-01  1.8444e-01\n",
      " -2.2056e-01  3.5941e-01 -4.9535e-01  1.9980e-01 -2.7222e-02 -1.0058e-01\n",
      " -2.5857e-02  4.3324e-01 -5.0012e-01 -5.7863e-02  5.5160e-01 -2.5393e-01\n",
      "  8.2712e-04 -4.0776e-01 -4.9360e-02 -7.2415e-01 -2.4047e-01  5.7750e-01\n",
      " -2.3353e-01 -1.0549e+00 -4.2750e-01 -1.0869e+00 -2.2777e-02  2.4416e-01\n",
      " -3.1473e-01  3.4935e-01  3.2641e-01  2.0999e-01  2.4311e-01  5.6577e-01\n",
      "  4.3662e-01  2.9002e-01 -1.0049e-01 -4.4168e-01 -5.9429e-01  1.1081e-02\n",
      " -3.4366e-01 -5.7492e-03 -4.3250e-03 -6.0442e-02 -4.2526e-01 -9.1973e-02\n",
      "  8.7490e-01  1.9249e-01  2.6095e-01  1.9257e-02 -2.0257e-01  3.4480e-01\n",
      " -1.6176e-01  7.7295e-02  2.6804e-01  4.9528e-01  3.6305e-01 -3.2937e-01\n",
      " -6.5333e-02 -5.1453e-02  2.8951e-02  8.6994e-01 -8.8409e-02  2.3938e-01\n",
      " -7.3066e-01 -1.6290e-01  5.7238e-01  1.2282e-01 -8.7052e-01 -2.2286e-01\n",
      " -5.6858e-01  4.0514e-02 -3.1478e-01 -8.6150e-01  6.6953e-01  1.8608e-01]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))\n",
    "\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))\n",
    "\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))\n",
    "\n",
    "print(doc[2].vector)\n",
    "\n",
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6334     0.18981   -0.53544   -0.52658   -0.30001    0.30559\n",
      " -0.49303    0.14636    0.012273   0.96802    0.0040354  0.25234\n",
      " -0.29864   -0.014646  -0.24905   -0.67125   -0.053366   0.59426\n",
      " -0.068034   0.10315    0.66759    0.024617  -0.37548    0.52557\n",
      "  0.054449  -0.36748   -0.28013    0.090898  -0.025687  -0.5947\n",
      " -0.24269    0.28603    0.686      0.29737    0.30422    0.69032\n",
      "  0.042784   0.023701  -0.57165    0.70581   -0.20813   -0.03204\n",
      " -0.12494   -0.42933    0.31271    0.30352    0.09421   -0.15493\n",
      "  0.071356   0.15022   -0.41792    0.066394  -0.034546  -0.45772\n",
      "  0.57177   -0.82755   -0.27885    0.71801   -0.12425    0.18551\n",
      "  0.41342   -0.53997    0.55864   -0.015805  -0.1074    -0.29981\n",
      " -0.17271    0.27066    0.043996   0.60107   -0.353      0.6831\n",
      "  0.20703    0.12068    0.24852   -0.15605    0.25812    0.007004\n",
      " -0.10741   -0.097053   0.085628   0.096307   0.20857   -0.23338\n",
      " -0.077905  -0.030906   1.0494     0.55368   -0.10703    0.052234\n",
      "  0.43407   -0.13926    0.38115    0.021104  -0.40922    0.35972\n",
      " -0.28898    0.30618    0.060807  -0.023517   0.58193   -0.3098\n",
      "  0.21013   -0.15557   -0.56913   -1.1364     0.36598   -0.032666\n",
      "  1.1926     0.12825   -0.090486  -0.47965   -0.61164   -0.16484\n",
      " -0.41134    0.19925    0.059183  -0.20842    0.45223    0.27697\n",
      " -0.20745    0.025404  -0.28874    0.040478  -0.22275   -0.43323\n",
      "  0.76957   -0.054327  -0.35213   -0.30842   -0.48791   -0.35564\n",
      "  0.19813   -0.094767  -0.50918    0.18763   -0.087555   0.37709\n",
      " -0.1322    -0.096913  -1.9102     0.55813    0.27391   -0.077744\n",
      " -0.43933   -0.10367   -0.24408    0.41869    0.11659    0.27454\n",
      "  0.81021   -0.11006    0.43131    0.29095   -0.49548   -0.31958\n",
      " -0.072506   0.020286   0.2179     0.22032   -0.29212    0.75639\n",
      "  0.13598    0.019736  -0.83104    0.22836   -0.28669   -1.0529\n",
      "  0.052771   0.41266    0.50149    0.5323     0.51573   -0.31806\n",
      " -0.4619     0.21739   -0.43584   -0.41382    0.042237  -0.57179\n",
      "  0.067623  -0.27854    0.090044   0.20633    0.024678  -0.57703\n",
      " -0.020183  -0.53147   -0.37548   -0.12795   -0.093662  -0.0061183\n",
      "  0.20221   -0.62296   -0.29746    0.26935    0.59009   -0.50382\n",
      " -0.69757    0.20157   -0.33592   -0.45766    0.14061    0.22982\n",
      "  0.044046   0.26386    0.02942    0.34095    1.1496    -0.15555\n",
      " -0.064071   0.30139    0.024211  -0.63515   -0.73347   -0.10346\n",
      " -0.22637   -0.056392  -0.16735   -0.097331  -0.19206   -0.18866\n",
      "  0.15116   -0.038048   0.70205    0.11586   -0.14813    0.0095166\n",
      " -0.33804   -0.10158   -0.23829   -0.22759    0.092504  -0.29839\n",
      " -0.39721    0.26092    0.34594   -0.47396   -0.25725   -0.19257\n",
      " -0.53071    0.1692    -0.47252   -0.17333   -0.40505    0.046446\n",
      " -0.04473    0.33555   -0.5693     0.31591   -0.21167   -0.31298\n",
      " -0.45923   -0.083091   0.086822   0.01264    0.43779    0.12651\n",
      "  0.30156    0.022061   0.26549   -0.29455   -0.14838    0.033692\n",
      " -0.37346   -0.075343  -0.56498   -0.24207   -0.69351   -0.20277\n",
      " -0.0081185  0.030971   0.53615   -0.16613   -0.84087    0.74661\n",
      "  0.029132   0.46936   -0.49755    0.40954   -0.022558   0.21497\n",
      " -0.049528  -0.039799   0.46165    0.26456    0.32985   -0.04219\n",
      " -0.099599  -0.17312   -0.476     -0.019048  -0.41888   -0.2685\n",
      " -0.65281    0.068773  -0.23881   -1.1784     0.25504    0.61171  ]\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "bananas_vector = doc[1].vector\n",
    "\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8456854224205017\n",
      "\n",
      "Part 2\n",
      "\n",
      "0.18317238986492157\n",
      "\n",
      "Part 3\n",
      "\n",
      "0.7541285157203674\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)\n",
    "\n",
    "print(\"\\nPart 2\\n\")\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)\n",
    "\n",
    "print(\"\\nPart 3\\n\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: love cats\n",
      "Match found: very happy\n",
      "Match found: very very happy\n",
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n",
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"LOWER\": \"cats\"}]\n",
    "matcher.add(\"LOVE_CATS\", [pattern])\n",
    "\n",
    "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
    "matcher.add(\"VERY_HAPPY\", [pattern])\n",
    "\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)\n",
    "\n",
    "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)\n",
    "\n",
    "matcher = spacy.matcher.PhraseMatcher(nlp.vocab)\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", [pattern])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"TEXT\": \"Amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"TEXT\": \"ad\"}, {\"TEXT\": \"-\"}, {\"TEXT\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "with open(\"countries.json\", encoding=\"utf8\") as file:\n",
    "    COUNTRIES = json.loads(file.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "matcher = spacy.matcher.PhraseMatcher(nlp.vocab)\n",
    "\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namibia --> Namibia\n",
      "South --> South Africa\n",
      "Cambodia --> Cambodia\n",
      "Kuwait --> Kuwait\n",
      "Somalia --> Somalia\n",
      "Haiti --> Haiti\n",
      "Mozambique --> Mozambique\n",
      "Somalia --> Somalia\n",
      "Rwanda --> Rwanda\n",
      "Singapore --> Singapore\n",
      "Sierra --> Sierra Leone\n",
      "Afghanistan --> Afghanistan\n",
      "Iraq --> Iraq\n",
      "Sudan --> Sudan\n",
      "Congo --> Congo\n",
      "Haiti --> Haiti\n",
      "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Somalia', 'GPE'), ('Rwanda', 'GPE'), ('Singapore', 'GPE'), ('Sierra Leone', 'GPE'), ('Afghanistan', 'GPE'), ('Iraq', 'GPE'), ('Sudan', 'GPE'), ('Congo', 'GPE'), ('Haiti', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "with open(\"countries.json\", encoding=\"utf8\") as file:\n",
    "    COUNTRIES = json.loads(file.read())\n",
    "\n",
    "with open(\"country_text.txt\", encoding=\"utf8\") as file:\n",
    "    TEXT = file.read()\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = spacy.matcher.PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = spacy.tokens.Span(doc, start, end, label=\"GPE\")\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "    span_root_head = span.root.head\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f847f2eab70>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f8480a7c890>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f847d264040>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f847e3d2fd0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f847d282210>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f847f30b990>)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rozdział 3 \n",
    "#3\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "display(nlp.pipeline)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Length component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "This document is 3 tokens long.\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "@spacy.language.Language.component(\"Length component\")\n",
    "def length_component_function(doc):\n",
    "    doc_length = len(doc)\n",
    "    print(f'This document is {doc_length} tokens long.')\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.add_pipe(\"Length component\", name=\"Length component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"Some random text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 6303828839600189595), ('Golden Retriever', 6303828839600189595)]\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = spacy.matcher.PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@spacy.language.Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [spacy.tokens.Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", name=\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n",
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "nlp = spacy.blank(\"en\")\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "spacy.tokens.Token.set_extension(\"is_country\", default=False, force=True)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])\n",
    "\n",
    "\n",
    "#Part 2\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
    "spacy.tokens.Token.set_extension(\"reversed\", getter=get_reversed, force=True)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n",
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "if spacy.tokens.Doc.has_extension(\"has_number\"):\n",
    "    spacy.tokens.Doc.remove_extension(\"has_number\")\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Register the Doc property extension \"has_number\" with the getter get_has_number\n",
    "spacy.tokens.Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)\n",
    "\n",
    "\n",
    "#Part 2\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Register the Span method extension \"to_html\" with the method to_html\n",
    "spacy.tokens.Span.set_extension(\"to_html\", method=to_html, force=True)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "spacy.tokens.Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url, force=True)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n",
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "with open(\"countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = spacy.matcher.PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "\n",
    "@spacy.language.Language.component(\"countries_component\")\n",
    "def countries_component_function(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [spacy.tokens.Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\", name=\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "spacy.tokens.Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['SANDWICH']\n",
      "['terrible', 'gettin']\n",
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) (The Arch Deluxe :P,) () (This morning,)\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "doc = nlp.pipe(TEXTS)\n",
    "for text in doc:\n",
    "    print([token.text for token in text if token.pos_ == \"ADJ\"])\n",
    "\n",
    "#Part 2\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)\n",
    "\n",
    "#Part 3\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\n",
      " — 'Metamorphosis' by Franz Kafka\n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing.\n",
      " — 'Moby-Dick or, The Whale' by Herman Melville\n",
      "\n",
      "It was the best of times, it was the worst of times.\n",
      " — 'A Tale of Two Cities' by Charles Dickens\n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\n",
      " — 'On the Road' by Jack Kerouac\n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      " — '1984' by George Orwell\n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing.\n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "with open(\"bookquotes.json\", encoding=\"utf8\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Register the Doc extension \"author\" (default None)\n",
    "spacy.tokens.Doc.set_extension(\"author\", default=None, force=True)\n",
    "\n",
    "# Register the Doc extension \"book\" (default None)\n",
    "spacy.tokens.Doc.set_extension(\"book\", default=None, force=True)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(f\"{doc.text}\\n — '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n",
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])\n",
    "\n",
    "#Part 2\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Disable the tagger and lemmatizer\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"lemmatizer\"]):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
