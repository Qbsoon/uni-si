{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie maszynowe i sieci neuronowe - teoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie maszynowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczenie maszynowe jest odpowiedzią na pytania: Czy komputery mogą wykraczać poza „to, co umiemy robić” i uczyć się samodzielnie sposobu wykonywania określonego zadania? Czy komputery mogą nas zaskakiwać? Czy komputer musi polegać na regułach wprowadzonych przez programistę? Czy może on samodzielnie utworzyć takie\n",
    "reguły, analizując dane?\n",
    "Pytania te doprowadziły to utworzenia nowego paradygmatu techniki programowania. \n",
    "\n",
    "W programowaniu klasycznym — paradygmacie symbolicznej sztucznej inteligencji — człowiek wprowadza reguły (tworzy program) oraz dane, które mają zostać przetworzone zgodnie z tymi regułami, a następnie oczekuje na uzyskanie danych wyjściowych. \n",
    "\n",
    "W przypadku uczenia maszynowego człowiek wprowadza dane, a także oczekiwane odpowiedzi, a komputer ma utworzyć reguły pozwalające na ich uzyskanie. Reguły te mogą zostać użyte w celu przetworzenia nowych danych i uzyskania oczekiwanych odpowiedzi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System uczenia maszynowego jest trenowany — nie programuje się go w sposób jawny. Przedstawia się mu wiele przykładów wyników, a system określa ich statystyczną strukturę pozwalającą na ustalenie reguł umożliwiających automatyzację procesu. W przypadku chęci zautomatyzowania np. oznaczania zdjęć z wakacji można pokusić się o utworzenie systemu uczenia maszynowego, który na podstawie wielu przykładów zdjęć z wakacji rozpoznanych przez ludzi może określić reguły przypisywania oznaczeń do zdjęć.\n",
    "\n",
    "Uczenie maszynowe zaczęło zyskiwać popularność dopiero w latach 90. XX w., ale stało się praktycznie od razu najszybciej rozwijającą się dziedziną uczenia maszynowego. Trend ten stał się możliwy dzięki dostępności coraz szybszego sprzętu komputerowego i pojawianiu się większych zbiorów danych. Uczenie maszynowe jest ściśle powiązane ze statystyką (dziedziną matematyki), ale między uczeniem maszynowym a statystyką istnieje kilka ważnych różnic. Uczenie maszynowe, w przeciwieństwie do statystyki, zwykle zajmuje się dużymi i rozbudowanymi zbiorami danych (takimi jak zbiór danych milionów obrazów, z których każdy składa się z dziesiątków tysięcy pikseli). W przypadku takich zbiorów danych klasyczna analiza statystyczna, taka jak wnioskowanie bayesowskie, nie jest praktycznym rozwiązaniem. W związku z tym uczenie maszynowe, a szczególnie uczenie głębokie jest mniej związane z matematyką, a bardziej z praktyczną wiedzą inżynierską. To praktyczna dyscyplina, w której twierdzenia sprawdza się w sposób empiryczny, a nie teoretyczny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu zdefiniowania uczenia głębokiego i zrozumienia różnicy między uczeniem głębokim a innymi technikami uczenia maszynowego musimy najpierw zrozumieć to, co tak naprawdę algorytmy uczenia maszynowego robią. \n",
    "\n",
    "Uczenie maszynowe prowadzi do odkrywania reguł przetwarzania danych na podstawie przykładowych oczekiwanych wyników. W związku z tym w celu przeprowadzenia uczenia maszynowego potrzebujemy trzech rzeczy. Są to:\n",
    "- **Dane wejściowe**. Jeżeli zadaniem jest rozpoznawanie mowy, to danymi mogą być pliki dźwiękowe z nagraniami wypowiedzi. W przypadku rozpoznawania obrazów danymi są pliki graficzne.\n",
    "- **Przykładowe oczekiwane wyniki**. W przypadku rozpoznawania mowy takie dane pochodzą z transkrypcji plików dźwiękowych wykonanej przez człowieka. Z kolei w przypadku rozpoznawania obrazów mogą to być etykiety obrazów, takie jak „pies” czy „kot”.\n",
    "- **Sposób pomiaru** tego, czy algorytm działa poprawnie. Jest to element niezbędny do określenia odległości między aktualnie generowanymi przez algorytm danymi wyjściowymi a oczekiwanymi danymi wyjściowymi. Wartość ta jest używana w roli informacji zwrotnej umożliwiającej dostrojenie działania algorytmu. Proces dostrajania algorytmu określamy mianem uczenia.\n",
    "\n",
    "Model uczenia maszynowego przekształca dane wejściowe w sensowne dane wyjściowe.Proces ten został „wyuczony” na drodze analizy znanych przykładowych danych wejściowych i wyjściowych. W związku z tym głównym problemem uczenia maszynowego i uczenia głębokiego jest właściwe przekształcenie danych. \n",
    "\n",
    "Proces ten ma na celu poznanie sposobu uzyskania oczekiwanej reprezentacji danych wejściowych. Zanim przejdziemy dalej, musimy wyjaśnić sobie, czym jest reprezentacja danych. Jest to tak naprawdę sposób przyglądania się danym — sposób ich przedstawiania i kodowania.\n",
    "\n",
    "Wszystkie algorytmy uczenia maszynowego zawierają mechanizmy automatycznego poszukiwania sposobu przekształcenia danych do reprezentacji, która ułatwi wykonanie danego zadania. Operacje takie mogą polegać na zmianie układu współrzędnych lub rzutowaniu liniowym, przesunięciu, a także mogą mieć charakter nieliniowy (mogą polegać np. na wybraniu wszystkich punktów spełniających warunek x > 0). \n",
    "\n",
    "Algorytmy uczenia maszynowego nie są na tyle kreatywne, aby samodzielnie wymyślać te transformacje. Korzystają one ze zdefiniowanego uprzednio zbioru operacji określanego mianem przestrzeni hipotez.\n",
    "\n",
    "Jak widać, uczenie maszynowe, technicznie rzecz biorąc, to poszukiwanie praktycznej reprezentacji danych wejściowych w ramach zdefiniowanej przestrzeni możliwości na podstawie sygnału informacji zwrotnej. Ta prosta technika umożliwia rozwiązywanie zaskakująco dużej liczby problemów — od rozpoznawania mowy do prowadzenia pojazdów autonomicznych.\n",
    "\n",
    "Teraz już wiesz, czym jest uczenie, a więc możemy przyjrzeć się temu, co sprawia, że uczenie głębokie jest czymś wyjątkowym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie maszynowe płytkie i głębokie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uczenie maszynowe\n",
    "\n",
    "Sztuczna inteligencja to narzędzia i algorytmy, które automatyzują procesy myślowe wykonywane przez ludzi. \n",
    "\n",
    "Uczenie maszynowe...\n",
    "\n",
    "<img src=\"./img/ai_ml_dl.png\" style=\"height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Płytkie uczenie maszynowe\n",
    "\n",
    "- modele probabilistyczne\n",
    "    - naiwny klasyfikator bayesowski\n",
    "    - regresja logistyczna\n",
    "    \n",
    "- metody jądrowe\n",
    "    - metoda wektorów nośnych (SVM)\n",
    "\n",
    "- drzewa decyzyjne\n",
    "    - drzewa losowe\n",
    "    - gradient boosting\n",
    "    \n",
    "- inne\n",
    "\n",
    "...\n",
    "\n",
    "[Introduction to Shallow Machine Learning](https://www.linkedin.com/pulse/introduction-shallow-machine-learning-ayman-mahmoud/)\n",
    "\n",
    "### Modele probabilistyczne\n",
    "Modelowanie probabilistyczne polega na zastosowaniu zasad statystyki w analizie danych. To jedna z pierwszych form uczenia maszynowego. Pomimo tego jest używana po dziś dzień.\n",
    "\n",
    "- **naiwny klasyfikator bayesowski**\n",
    "\n",
    "Naiwny klasyfikator bayesowski jest klasyfikatorem uczenia maszynowego, który stosuje twierdzenie Bayesa i zakłada, że wszystkie parametry charakteryzujące dane wejściowe są niezależne (to dość „naiwne” założenie, z którego wzięła się nazwa klasyfikatora). Ta metoda analizy danych jest starsza od komputerów (była stosowana ręcznie najprawdopodobniej już w latach 50. XX w.). Twierdzenie Bayesa i podstawy statystyki mające korzenie w XVIII w. to wszystko, czego potrzebujesz do zaimplementowania naiwnego klasyfikatora bayesowskiego.\n",
    "\n",
    "- **regresja logistyczna**\n",
    "\n",
    "Z naiwnym klasyfikatorem bayesowski związany jest model regresji logistycznej, który jest uważany za podstawowy model uczenia maszynowego. Pomimo swojej nazwy jest to algorytm klasyfikacji, a nie regresji. Regresja logistyczna, podobnie do naiwnego klasyfikatora bayesowskiego, pojawiła się dużo wcześniej niż komputery, ale wciąż jest stosowana z powodu swojej prostoty i uniwersalności. Często jest to pierwszy model używany przez analityka w celu rozpoznania stojącego przed nim zadania.\n",
    "\n",
    "### Metody jądrowe\n",
    "\n",
    "W latach 90. XX w. sieci neuronowe zaczęły zyskiwać szacunek badaczy z powodu swoich pierwszych sukcesów. Spowodowało to ich dalszy rozwój i powstanie metod jądrowych.\n",
    "\n",
    "- **metoda wektorów nośnych (SVM)**\n",
    "\n",
    "Metody jądrowe to grupa algorytmów klasyfikacji, są one lepiej znane jako maszyny wektorów nośnych (maszyny SVM). Implementacja maszyn SVM została opracowana przez Vladimira Vapnika i Corrinę Cortes we wczesnych latach 90. w firmie Bell Labs. Opublikowano ją w 1995 r., ale starsza liniowa wersja tego algorytmu została opracowana przez Vapkina i Alexeya Chervonenkisa już w 1963 r.\n",
    "\n",
    "Maszyny SVM próbują rozwiązać problem klasyfikacji, znajdując odpowiednie granice decyzyjne oddzielające od siebie dwa zbiory punków należące do dwóch różnych kategorii.\n",
    "Granicę decyzyjną można traktować jako linię lub powierzchnię dzielącą treningowy zbiór danych na przestrzenie dwóch kategorii. \n",
    "\n",
    "W celu dokonania klasyfikacji nowych elementów zbioru danych wystarczy sprawdzić, po której stronie granicy decyzyjnej się znajdują.\n",
    "\n",
    "Maszyny wektorów nośnych znajdują takie granice, wykonując dwie operacje:\n",
    "\n",
    "1. Dane są mapowane na nową reprezentację o wysokiej liczbie wymiarów, w której granice decyzyjne można wyrazić w formie hiperpłaszczyzny (jeżeli dane mają tylko dwa wymiary — to hiper- płaszczyzna jest linią).\n",
    "2. Dobra granica decyzyjna (hiperpłaszczyzna rozdzielająca) jest określana poprzez trenowanie algorytmu w celu zmaksymalizowania odległości między hiperpłaszczyzną a najbliższymi punktami danych z określonej klasy (proces ten nazywamy maksymalizacją marginesu). Dzięki temu granica może zostać uogólniona i sprawdzać się również w przypadku nowych próbek, które nie znajdują się w treningowym zbiorze danych.\n",
    "\n",
    "Technika mapowania danych na wysokowymiarową reprezentację, w której problem klasyfikacji staje się prostszy, może wyglądać dobrze na papierze, ale w praktyce może okazać się niemożliwa do zastosowania z powodu skomplikowania obliczeń. Problem ten rozwiązuje się za pomocą sztuczki jądra (ang. kernel trick). Właśnie od niej swoją nazwę wzięły metody jądrowe. Polega ona na tym, że w celu znalezienia dobrych hiperpłaszczyzn decyzyjnych w nowej przestrzeni nie trzeba jawnie obliczać współrzędnych punktów. Wystarczy obliczyć odległości między parami punktów umieszczonych w nowej przestrzeni, co można zrobić wydajnie przy użyciu funkcji jądra. Funkcja jądra jest operacją polegającą na przypisywaniu dowolnym dwóm punktom początkowej przestrzeni odległości między nimi w docelowej hiperprzestrzeni przy całkowitym pominięciu konieczności jawnego obliczania nowej reprezentacji. Funkcje jądra są zwykle tworzone w sposób ręczny — nie generuje się ich na podstawie danych — w przypadku maszyn wektorów nośnych w procesie uczenia ustalana jest tylko hiper- płaszczyzna rozdzielająca klasy.\n",
    "\n",
    "Początkowo maszyny SVM charakteryzowały się ogromną wydajnością przy prostych problemach klasyfikacji i były jedną z kilku metod uczenia maszynowego, za którymi stało solidne zaplecze teoretyczne i poważna analiza matematyczna, co sprawiało, że łatwo było je zrozumieć i zinterpretować. Zalety te przyczyniły się do długotrwałej popularności maszyn wektorów wspierających.\n",
    "\n",
    "Niestety maszyny SVM są trudne do skalowania w przypadku dużych zbiorów danych i nie dają dobrych wyników w przypadku problemów percepcyjnych, takich jak klasyfikacja obrazu. Algorytm maszyny CVM jest metodą płytką. Użycie go w celu rozwiązania problemu percepcyjnego wymaga wstępnego ręcznego uzyskania właściwej reprezentacji danych (proces ten jest określany mianem obróbki cech), co jest trudne i pracochłonne.\n",
    "\n",
    "### Drzewa decyzyjne\n",
    "Drzewa decyzyjne są strukturą przypominającą drzewa. Umożliwiają one klasyfikację danych wejściowych i przewidywanie wartości wyjściowych na podstawie danych wejściowych. Są łatwe do przedstawienia w formie graficznej i do zinterpretowania. Zainteresowanie nimi wzrosło w pierwszej dekadzie lat dwutysięcznych, a do 2010 r. korzystano z nich częściej niż z metod jądrowych.\n",
    "\n",
    "- **drzewa losowe**\n",
    "\n",
    "Szczególnie popularne stały się algorytmy lasów losowych, które wprowadziły mocną, praktyczną technikę trenowania drzew decyzyjnych poprzez tworzenie wielu wyspecjalizowanych drzew decyzyjnych i łączenie zwracanych przez nie wyników. Lasy losowe stosuje się\n",
    "do rozwiązywania różnych problemów — można uznać je za drugi najlepszy algorytm przeznaczony do rozwiązywania zadań płytkiego uczenia maszynowego. Od 2010 r. lasy losowe stały się najpopularniejszym algorytmem używanym do rozwiązywania problemów analitycznych. Trwało to aż do 2014 r., gdy pozycję lasów losowych\n",
    "zajęły algorytmy maszyn wzmacnianych gradientowo.\n",
    "    \n",
    "- **gradient boosting**\n",
    "\n",
    "Gradientowe wzmacnianie maszyn to, podobnie jak lasy losowe, technika oparta na łączeniu ze sobą wielu słabych modeli (ogólnie rzecz biorąc, modelami tymi są drzewa decyzyjne). W technice tej zastosowano wzmacnianie gradientowe — rozwiązanie umożliwiające ulepszenie każdego modelu uczenia maszynowego poprzez iteracyjne trenowanie nowych modeli specjalizujących się w rozwiązywaniu sytuacji sprawiających problemy poprzednim modelom. Zastosowanie drzew decyzyjnych w połączeniu ze wzmacnianiem gradientowym pozwoliło na uzyskanie modeli, które zwykle sprawdzają się lepiej od techniki lasów losowych, ale wciąż charakteryzują się podobnymi do niej właściwościami. Jest to jeden z najlepszych, o ile nie najlepszy, algorytm stosowany współcześnie do pracy z danymi innymi niż percepcyjne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powrót do sieci neuronowych\n",
    "\n",
    "W 2010 r. sieci neuronowe były praktycznie zupełnie zapomniane przez społeczność naukowców, ale pomimo tego zdarzały się osoby, które z nich korzystały i którym udało się dokonać czegoś przełomowego. Takimi osobami były: Geoffrey Hinton (Uniwersytet w Toronto), Yoshua Bengio (Uniwersytet Montrealski) i Yann LeCun (Uniwersytet Nowojorski). Z sieci neuronowych korzystało również szwedzkie laboratorium IDSIA. \n",
    "\n",
    "W 2011 r. Dan Ciresan z IDSIA zaczął wygrywać akademickie konkursy klasyfikacji obrazu za pomocą sieci neuronowych trenowanych na układach graficznych — był to pierwszy praktyczny sukces współczesnego uczenia maszynowego. Przełomowym wydarzeniem było wejście grupy Geoffreya Hintona do konkursu klasyfikacji obrazu ImageNet w 2012 r. Konkurs ten był wówczas bardzo trudny. Wymagał od uczestników klasyfikacji kolorowych obrazów o wysokiej rozdzielczości na 1000 różnych kategorii po wytrenowaniu na 1,4 miliona przykładowych obrazów. W 2011 r. mierzona podczas konkursu dokładność klasyfikacji wynosiła zaledwie 74,3%. W 2012 r. zespół dowodzony przez Alexa Krizhevsky’ego, korzystający z doradztwa Geoffreya Hintona, uzyskał wynik na poziomie 83,6%. Był to ogromny przełom. Odtąd konkurs został zdominowany przez konwolucyjne sieci neuronowe. Zwycięzca konkursu w 2015 r. uzyskał dokładność na poziomie 96,4%, a klasyfikacja obrazów konkursu ImageNet została uznana za problem, który został całkowicie rozwiązany. \n",
    "\n",
    "Od 2012 r. głębokie konwolucyjne sieci neuronowe (zwane w skrócie sieciami konwolucyjnymi) stały się najpopularniejszym algorytmem przetwarzania obrazu. W praktyce sprawdzają się one podczas pracy nad wszystkimi zadaniami percepcyjnymi. Na najważniejszych konferencjach związanych z komputerowym przetwarzaniem obrazu w latach 2015 – 2016 trudno było znaleźć prezentacje, w których w jakiś sposób nie zahaczano o tematykę sieci konwolucyjnych. Równocześnie uczenie głębokie zyskało wiele innych zastosowań. Zaczęto go używać między innymi podczas przetwarzania języka naturalnego. W wielu zastosowaniach sieci te całkowicie zastąpiły maszyny SVM i drzewa decyzyjne. Przez wiele lat Europejska Organizacja Badań Jądrowych (CERN) korzystała z metod opartych na drzewach decyzyjnych podczas analizy danych zbieranych przez czujnik ATLAS Wielkiego Zderzacza Hadronów (LHC). Jednak organizacja ta zdecydowała się zmienić algorytm na głębokie sieci neuronowe oparte na pakiecie Keras, a to z powodu ich wyższej skuteczności i łatwości trenowania na dużych zbiorach danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Głębokie uczenie maszynowe\n",
    "\n",
    "Główna przyczyną tak szybkiego wzrostu popularności uczenia głębokiego jest większa dokładność tej techniki podczas rozwiązywania wielu problemów, ale nie jest to jedyny powód. Uczenie głębokie sprawia również, że rozwiązywanie problemów staje się o wiele łatwiejsze z powodu pełnej automatyzacji najważniejszego etapu uczenia maszynowego: obróbki cech.\n",
    "\n",
    "Wcześniejsze techniki uczenia maszynowego — techniki uczenia płytkiego — doprowadzały do przekształcenia danych wejściowych w jedną lub w dwie kolejne przestrzenie reprezentacji poprzez proste transformacje, takie jak wysokowymiarowe nieliniowe rzutowanie (maszyny SVM) lub tworzenie drzew decyzyjnych. Niestety zmodyfikowana reprezentacja złożonych problemów, ogólnie rzecz biorąc, nie może zostać uzyskana za pomocą tych technik. W celu wygenerowania danych wejściowych, które mogłyby być akceptowane przez te metody, należy ręcznie zaprojektować odpowiednie warstwy reprezentacji danych, a więc przeprowadzić tzw. obróbkę cech. W uczeniu głębokim etap ten jest całkowicie zautomatyzowany — nie ma konieczności ręcznej obróbki cech. Ułatwia to znacznie przepływ roboczy całego procesu — skomplikowane wieloetapowe operacje zastępuje jeden prosty model uczenia maszynowego.\n",
    "\n",
    "Uczenie głębokie polega na zastosowaniu wielu kolejnych warstw reprezentacji. Czy w związku z tym to samo osiągnie się, stosując kilkukrotnie metody płytkie? W praktyce stosowanie kolejnych etapów metod uczenia płytkiego prowadzi do coraz mniejszej poprawy uzyskiwanych wyników, ponieważ optymalna pierwsza warstwa reprezentacji trójwarstwowego modelu nie jest optymalną pierwszą warstwą jednowarstwowego lub dwuwarstwowego modelu. Uczenie głębokie umożliwia modelowi jednoczesne łączne przetwarzanie wszystkich warstw reprezentacji. W takim przypadku modyfikacja jednej wewnętrznej cechy modelu powoduje automatyczną adaptację do tej zmiany pozostałych cech, bez potrzeby wykonywania operacji przez użytkownika. Wszystko to nadzoruje jeden sygnał zwrotny: każda zmiana modelu wpływa na osiąganie przez niego celu. To o wiele lepsze rozwiązanie od tworzenia stosów modeli płytki, ponieważ umożliwia trenowanie modelu na złożonej abstrakcyjnej reprezentacji bez potrzeby dzielenia jej na warstwy pośrednie, które są modyfikowane niezależnie od siebie.\n",
    "\n",
    "Istnieją dwie podstawowe cechy trenowania modeli uczenia głębokiego: tworzenie bardziej skomplikowanych reprezentacji w sposób inkrementalny (warstwa po warstwie) i to, że te pośrednie reprezentacje są przetwarzane jednocześnie — aktualizacja każdej warstwy powoduje automatyczną modyfikację warstw znajdujących się niżej i wyżej. Dzięki tym dwóm cechom uczenie głębokie stało się popularniejsze od innych technik uczenia maszynowego.\n",
    "\n",
    "Konkursy programistyczno-analityczne zostały zdominowane przez dwie metody: maszyny wzmacniane gradientowo i uczenie głębokie. Maszyny wzmacniane gradientowo są używane w celu rozwiązywania problemów, w których dostępne są ustrukturyzowane dane, a uczenie głębokie jest stosowane w przypadku problemów percepcyjnych, takich jak klasyfikacja obrazów.\n",
    "\n",
    "Głębokie uczenie maszynowe (w przypadku problemów percepcyjnych) i maszyny wzmacniane gradientowo (w przypadku płytkich problemów) to dwie techniki, które warto poznać jak najlepiej,\n",
    "\n",
    "Głębokie ucznie maszynowe opiera się na sztucznych sieciach neuronowych (ang. *Artificial Neural Network*). Dziś w zasadzie zamiennie używa się terminów głęboka sieć neuronowa i sieć neuronowa. Podstawowym elementem sieci neuronowej jest sztuczny neuron, którego zadaniem jest znalezienie odpowiedniej funkcji odwzorowującej wejście $X$ na wyjście $y$.\n",
    "\n",
    "<img src=\"./img/neuron.png\">\n",
    "\n",
    "Sieć neuronowa to połączenie takich prostych neuronów:\n",
    "\n",
    "<img src=\"./img/siec_neuronowa.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można widzieć sieć neuronową jako \"black box\", której celem jest odwzorowanie wejścia $X$ na wyjęcie $y$.\n",
    "\n",
    "<img src=\"./img/siec_neuronowa2.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span t=\"c\"></span>\n",
    "\n",
    "$X$ może być dowolnym wektorem cech postaci numerycznej. Podobnie $y$, również musi być postaci liczbowej, np. $1$ będzie symbolizować popyt, a $0$ brak popytu.\n",
    "\n",
    "- sieci gęste\n",
    "- sieci rekurencyjne\n",
    "- sieci konwulacyjne\n",
    "\n",
    "Zbiory treniongowy i testowy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozwój sieci neuronowych\n",
    "Dwie podstawowe techniki związane z uczeniem głębokim i przetwarzaniem obrazu — konwolucyjne sieci neuronowe i algorytm propagacji wstecznej — były dobrze znane już w 1989 r. Algorytm LSTM (ang. Long Short Term Memory), będący podstawowym algorytmem uczenia głębokiego przeznaczonym do pracy z danymi szeregu czasowego, został opracowany w 1997 r. i praktycznie od tego czasu nie modyfikowano go w sposób znaczący. Dlaczego uczenie głębokie stało się popularne dopiero po 2012 r.? Co się zmieniło przez te dwie dekady?\n",
    "\n",
    "Ogólnie rzecz biorąc, rozwój uczenia maszynowego jest napędzany przez postęp technologiczny:\n",
    "- lepszy sprzęt,\n",
    "- rozwój zbiorów danych i metod porównawczych,\n",
    "- lepsze algorytmy.\n",
    "\n",
    "Rozwój tej dziedziny bardziej od teorii napędzają eksperymentalne odkrycia, a więc rozwój algorytmów jest możliwy tylko wtedy, gdy pojawiają się dane i rozwiązania sprzętowe, które pozwalają na wypróbowanie nowych pomysłów lub przeskalowanie\n",
    "znanych od dawna metod. Uczenie maszynowe to nie matematyka i fizyka. Tutaj do większości odkryć nie dochodzi przy użyciu długopisu i papieru. Uczenie maszynowe to inżynieria.\n",
    "\n",
    "Głównym problemem lat dziewięćdziesiątych i dwutysięcznych był dostęp do danych i sprzętu, ale w międzyczasie rozwinął się internet i powstały układy graficzne o wysokiej wydajności, które miały zaspokoić potrzeby graczy.\n",
    "\n",
    "W latach dwutysięcznych firmy takie jak NVIDIA i AMD zainwestowały miliardy dolarów w rozwój szybkich układów graficznych zdolnych do sprawnego równoległego przetwarzania wielu potoków danych.\n",
    "Miało to zaspokoić rosnące potrzeby coraz bardziej skomplikowanej grafiki trójwymiarowych gier. Inwestycja ta przyniosła korzyści również dla świata nauki. W 2007 r. NVIDIA uruchomiła projekt CUDA (https://developer.nvidia.com/about-cuda) — udostępniła interfejs programistyczny swoich procesorów graficznych. W obliczeniach przeprowadzanych równolegle kilka układów graficznych mogło wówczas zastąpić cały klaster procesorów ogólnego przeznaczenia. Głębokie sieci neuronowe składają się głównie z wielu operacji mnożenia małych macierzy i można je skutecznie zrównoleglić. Około 2011 r. zaczęto pracę nad napisaniem implementacji sieci neuronowych korzystających z technologii CUDA. Pionierami w tej dziedzinie byli Dan Ciresan i Alex Krizhevsky.\n",
    "\n",
    "Przemysł uczenia głębokiego chce wykroczyć poza układy graficzne i zainwestować w wysoce wyspecjalizowane wydajne czipy utworzone wyłącznie z myślą o uczeniu głębokim. Firma Google podczas corocznej konwencji I/O pokazała procesor przetwarzający tensory (układ TPU). Projekt tego czipa został opracowany od podstaw i zoptymalizowany pod kątem sieci neuronowych. Jest znacznie szybszy od najlepszych dostępnych układów graficznych, a dodatkowo bardziej energooszczędny.\n",
    "\n",
    "Poza problemami związanymi z danymi i sprzętem do końca lat dwutysięcznych problemem był również brak niezawodnego sposobu trenowania bardzo głębokich sieci neuronowych. Przyczyniało się to do tego, że sieci neuronowe były wciąż dość płytkie i korzystały z jedno- lub dwuwarstwowej reprezentacji, a więc nie mogły mieć przewagi nad bardziej dopracowanymi metodami uczenia płytkiego, takimi jak maszyny SVM i lasy losowe. Największy problem dotyczył propagacji gradientu przez głębokie stosy warstw. Sygnał sprzężenia zwrotnego używany podczas trenowania sieci neuronowych rozpływał się wraz ze wzrostem liczby warstw.\n",
    "\n",
    "Sytuacja ta uległa zmianie w latach 2009 – 2010 wraz z wprowadzeniem prostych, ale skutecznych poprawek algorytmu, umożliwiających lepszą propagację gradientu.\n",
    "Były to:\n",
    "- lepsze funkcje aktywacji warstw neuronów,\n",
    "- lepsze schematy inicjacji wag rozpoczynające się od wstępnego trenowania poszczególnych warstw, (rozwiązanie to jest rzadko stosowane w praktyce),\n",
    "- lepsze schematy optymalizacji, takie jak RMSProp i Adam.\n",
    "\n",
    "Po wprowadzeniu tych ulepszeń możliwe stało się trenowanie modeli składających się z 10 lub więcej warstw. Dopiero wtedy uczenie głębokie zaczęło pokazywać swój potencjał.\n",
    "W latach 2014, 2015 i 2016 odkryto jeszcze bardziej zaawansowane metody wspierania propagacji gradientu, takie jak normalizacja wsadowa, połączenia szczątkowe i konwolucje oddzielane w zależności od głębokości. Dzisiaj możemy trenować od podstaw modele składające się z tysięcy warstw.\n",
    "\n",
    "Uczenie głębokie jest popularne od zaledwie kilku lat i wciąż nie znamy pełni możliwości tej technologii. Co miesiąc dowiadujemy się o nowych zastosowaniach i modyfikacjach pokonujących wcześniejsze ograniczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie nadzorowane i nienadzorowane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uczenie nadzorowane\n",
    "\n",
    "<span t=\"l1\">W uczeniu nadzorowanym (aka uczeniu z nauczycielem) mamy wejście $X$, które jest wektorem cech opisujących dany obiekt, oraz wyjście $y$.</span> Wyjście może być różnie zdefiniowane.\n",
    "\n",
    "<div t=\"t\">\n",
    "    \n",
    "|    \t|                     $X$                     \t|                     $y$                   |\n",
    "|:--:\t|:-----------------------------------------:\t|:----------------------------------------:\t|\n",
    "| 1. \t|          cechy opisujące działki          \t|                   cena                   \t|\n",
    "| 2. \t|                 plik audio                \t|            transkrypcja tekstu           \t|\n",
    "| 3. \t|                tekst polski               \t|          tłumaczenie angielskie          \t|\n",
    "| 4. \t|        dane o użytkowniku, reklama        \t|       1 - kliknie, 0 - nie kliknie       \t|\n",
    "| 5. \t|               treści emaila               \t|         1 - spam, 0 - brak spamu         \t|\n",
    "| 6. \t|          obraz z kamery telefonu          \t| 1 - odblokowanie, 0 - blokada            \t|\n",
    "| 7. \t| obraz produktu z kamery przemysłowej  \t    | 1 - brak defektu, 0 - defekt             \t|\n",
    "| 8. \t|   tekst komentarza dotyczący produktu  \t    | 1 - ocena pozytywna, 0 - ocena negatywna \t|\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uczenie nienadzorowane\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dane strukturalne i niestrukturalne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane strukturalne\n",
    "\n",
    "Dane strukturalne to w zasadzie dane tabelaryczne, czyli bazy danych, arkusze kalkulacyjne, itp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane niestrukturalne\n",
    "\n",
    "Dane niestrukturalne (nieustrukturyzowane) to obrazy, tekst oraz dzwięk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie maszynowe i sieci neuronowe - praktyka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 09:55:06.603286: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-16 09:55:06.687683: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-16 09:55:06.688040: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-16 09:55:06.688093: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-16 09:55:06.708053: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-16 09:55:06.709058: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 09:55:08.405965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pierwszy przykład sieci neuronowej\n",
    "\n",
    "W zaprezentowanym przykładzie spróbujemy rozwiązać problem klasyfikacji obrazów w skali szarości przedstawiających ręcznie zapisane cyfry (obrazy te mają rozdzielczość 28x28 pikseli). Chcemy podzielić je na 10 kategorii (cyfry od 0 do 9). Będziemy korzystać ze zbioru danych MNIST, który jest uznawany przez środowisko analityków za zbiór klasyczny. Istnieje on tak długo, jak długa jest historia uczenia maszynowego. Zbiór ten zawiera 60 000 obrazów treningowych oraz 10 000 obrazów testowych. Został on utworzony przez Narodowy Instytut Standaryzacji i Technologii (NIST) w latach 80. ubiegłego wieku. Rozwiązanie wspomnianego problemu można porównać do wyświetlenia napisu „Witaj, świecie!” podczas nauki nowego języka programowania. Zbiór ten jest również używany w celu sprawdzania tego, czy algorytm działa poprawnie. Jeżeli zaczniesz zawodowo zajmować się uczeniem maszynowym, to odkryjesz, że zbiór MNIST pojawia się ciągle w różnych pracach naukowych, artykułach publikowanych w internecie itd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternative text](./img/MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbiór danych MNIST jest dołączony do pakietu Keras w formie czterech tablic Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tablice train_images i train_labels tworzą treningowy zbiór danych. Będzie on używany podczas trenowania modelu. Do testowania posłuży nam testowy zbiór danych, składający się z tablic test_images i test_labels. Obrazy są zakodowane w formie tablic Numpy, a etykiety mają formę tablicy cyfr (od 0 do 9). Do każdego obrazu przypisana jest tylko jedna etykieta.\n",
    "\n",
    "![alternative text](./img/MNIST_label.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się treningowemu zbiorowi danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A teraz zobaczmy, jak wyglądają dane testowe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy pracować według następującego przepływu roboczego: najpierw będziemy trenować sieć neuronową na danych treningowych: train_images i train_labels. Sieć nauczy się kojarzyć obrazy i etykiety. Następnie nasza sieć wygeneruje przewidywania dotyczące zbioru test_images, a uzyskane wyniki porównamy z etykietami test_labels.\n",
    "\n",
    "Zbudujmy naszą sieć. Na obecnym etapie nie musisz jeszcze rozumieć wszystkiego, co się dzieje w tym przykładzie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Głównym blokiem składowym sieci neuronowej jest warstwa (ang. layer). Jest to moduł przetwarzania danych, który można traktować jako filtr danych. Dane wychodzące z filtra mają bardziej przydatną formę od danych do niego wchodzących. Niektóre warstwy dokonują ekstrakcji reprezentacji kierowanych do nich danych — reprezentacje te powinny ułatwiać rozwiązanie problemu, z którym się zmagamy. Większość uczenia głębokiego składa się z łączenia ze sobą prostych warstw w celu zaimplementowania progresywnej destylacji danych. Model uczenia głębokiego jest jak sito przetwarzające dane składające się z coraz drobniejszych siatek — warstw.\n",
    "\n",
    "Nasza sieć składa się z sekwencji dwóch warstw Dense, które są ze sobą połączone w sposób gęsty (dochodzi tu do gęstego połączenia). Druga warstwa jest dziesięcioelementową warstwą softmax — warstwa ta zwróci tablicę 10 wartości prawdopodobieństwa (suma wszystkich tych wartości jest równa 1). Każdy z tych wyników określa prawdopodobieństwo tego, że na danym obrazie przedstawiono daną cyfrę (obraz może przedstawiać jedną z dziesięciu cyfr).\n",
    "\n",
    "Na etapie kompilacji musimy określić jeszcze trzy rzeczy w celu przygotowania sieci do trenowania. Są to:\n",
    "\n",
    "* Funkcja straty — funkcja ta definiuje sposób pomiaru wydajności sieci podczas przetwarzania treningowego zbioru danych, a więc pozwala na dostrajanie parametrów sieci we właściwym kierunku.\n",
    "* Optymalizator — mechanizm dostrajania sieci na podstawie danych zwracanych przez funkcje straty.\n",
    "* Metryki monitorowane podczas trenowania i testowania — tutaj interesuje nas jedynie dokładność (część obrazów, która została właściwie sklasyfikowana).\n",
    "\n",
    "W przyszłości omówimy cel stosowania funkcji straty i optymalizatora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Zanim rozpoczniemy trenowanie, zmienimy kształt danych tak, aby przyjęły kształt oczekiwany przez sieć, i przeskalujemy je do wartości z zakresu [0, 1]. Początkowo nasze obrazy treningowe były zapisywane w postaci macierzy o wymiarach (60000, 28, 28), zawierającej wartości z zakresu [0, 255], i typie uint8. Przekształcamy je w tablicę typu float32 o wymiarach (60000, 28 * 28), zawierającą wartości od 0 do 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Musimy dodatkowo zakodować etykiety za pomocą kategorii:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 09:55:28.364371: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 5s 9ms/step - loss: 0.2617 - accuracy: 0.9257\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1066 - accuracy: 0.9687\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0700 - accuracy: 0.9786\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0514 - accuracy: 0.9846\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0384 - accuracy: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f541806cb10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podczas trenowania wyświetlane są dwie wartości: strata sieci i jej dokładność (obie wartości dotyczą treningowego zbioru danych).\n",
    "\n",
    "Podczas trenowania szybko osiągamy dokładność 0,988 (98,8%). Teraz możemy sprawdzić dokładność przetwarzania testowego zbioru danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  1/313 [..............................] - ETA: 44s - loss: 0.0101 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 09:55:50.130935: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 31360000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0678 - accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.977400004863739\n"
     ]
    }
   ],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "W przypadku testowego zbioru danych uzyskaliśmy dokładność na poziomie 0,977 (97,7%), a więc wartość nieco niższą niż dla zbioru treningowego. Różnica między tymi wartościami wynika z nadmiernego dopasowania. Modele uczenia maszynowego mają tendencję do niższej dokładności przetwarzania nowych danych, niż to miało miejsce w przypadku danych treningowych.\n",
    "\n",
    "Zobaczyliśmy, że zbudowanie i wytrenowanie sieci neuronowej klasyfikującej zapis ręczny cyfr może zająć mniej niż 20 linii kodu Pythona. W przyszłości omówimy, jakie operacje są wykonywane w tle. Dowiesz się, czym są tensory, obiekty przeznaczone do przechowywania danych sieci i operacje na tensorach. Poznasz budowę warstw sieci i algorytm spadku gradientowego, który umożliwia sieci uczenie się na podstawie treningowego zbioru danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprezentacja danych w sieciach neuronowych\n",
    "Na początku zaprezentowanego przykładu dane były przechowywane w wielowymiarowych tablicach Numpy określanych mianem tensorów. Ogólnie rzecz biorąc, tensory są podstawową strukturą danych we wszystkich współczesnych systemach uczenia maszynowego. Tensory są tak ważne, że firma Google zdecydowała się na ich cześć nadać swojej bibliotece uczenia maszynowego nazwę TensorFlow. Czym tak naprawdę jest tensor?\n",
    "\n",
    "Tensor jest strukturą danych — praktycznie zawsze są to dane numeryczne, a więc jest to struktura przeznaczona do przechowywania liczb. Prawdopodobnie wiesz, czym są macierze. Można je określić mianem dwuwymiarowych tensorów. Tensory są uogólnieniem macierzy w przestrzeniach o dowolnej liczbie wymiarów (w kontekście tensorów wymiary nazywa się osiami)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ciąg dalszy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadania do wykonania\n",
    "\n",
    "1. Uzupełnić modele płytkiego uczenia maszynowego, dodać schematy działania, przykłady zastosowania...\n",
    "\n",
    "- modele probabilistyczne\n",
    "    - naiwny klasyfikator bayesowski\n",
    "    - regresja logistyczna\n",
    "    \n",
    "- metody jądrowe\n",
    "    - metoda wektorów nośnych (SVM)\n",
    "\n",
    "- drzewa decyzyjne\n",
    "    - drzewa losowe\n",
    "    - gradient boosting\n",
    "    \n",
    "- inne\n",
    "\n",
    "2. Uzupełnić materiały o własne przemyślenia, wnioski, grafiki, schematy..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
