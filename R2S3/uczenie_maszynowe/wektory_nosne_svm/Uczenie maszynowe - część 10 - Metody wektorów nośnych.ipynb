{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie maszynowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metody wektorów nośnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zastosowanie: klasyfikacja.\n",
    "\n",
    "Metoda: uczenie nadzorowane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maszyna wektorów nośnych (SVM)\n",
    "\n",
    "Maszyna wektorów nośnych to algorytm klasyfikacyjny, którego celem jest wyznaczenie granicy decyzyjnej w postaci hiperpowierzchni. Położenie obiektów względem tej hiperpowierzchni determinuje przypisanie ich do odpowiednich klas. SVM powstało jako odpowiedź na ograniczenia innych algorytmów, takich jak klasyfikator maksymalizujący margines oraz klasyfikator wektorów nośnych (klasyfikator miękkiego marginesu). Aby w pełni zrozumieć działanie SVM, warto najpierw poznać te algorytmy, na których bazuje.\n",
    "\n",
    "Wszystkie trzy algorytmy wykorzystują pojęcie marginesu, czyli odległości między granicą decyzyjną a najbliższymi obiektami treningowymi. W każdym przypadku dąży się do maksymalizacji marginesu, ale różnią się one sposobem definiowania granicy decyzyjnej:\n",
    "\n",
    "1. **Klasyfikator maksymalizujący margines**:\n",
    "   - Wyznacza hiperpłaszczyznę rozdzielającą obiekty na dwie klasy.\n",
    "   - Może być używany tylko dla danych liniowo separowalnych, co znacznie ogranicza jego zastosowanie.\n",
    "\n",
    "2. **Klasyfikator miękkiego marginesu (soft margin)**:\n",
    "   - Bazuje na koncepcji liniowej separacji, ale pozwala na obecność pewnych obiektów w obszarze marginesu, a nawet błędne klasyfikacje.\n",
    "   - Dzięki temu może być stosowany do danych nieliniowo separowalnych, choć wciąż używa liniowej granicy decyzyjnej. Dla danych wyraźnie nieliniowych może generować duże błędy klasyfikacyjne.\n",
    "\n",
    "3. **Maszyna wektorów nośnych (SVM)**:\n",
    "   - Rozszerza możliwości klasyfikatora miękkiego marginesu, umożliwiając tworzenie nieliniowych granic decyzyjnych dzięki zastosowaniu odpowiednich przekształceń przestrzeni wejściowej (tzw. funkcji jądrowych).\n",
    "   - W razie potrzeby SVM może również korzystać z miękkiego marginesu.\n",
    "\n",
    "### Geometryczna interpretacja\n",
    "\n",
    "Działanie algorytmów można łatwiej zrozumieć, analizując ich geometryczną interpretację. W tej perspektywie obiekty w zbiorze treningowym są punktami w przestrzeni wejściowej oznaczonymi etykietami \\(y^{(j)} \\in \\{-1, 1\\}\\), gdzie \\(j = 1, \\dots, N\\). Zbiór treningowy można zapisać jako:\n",
    "\n",
    "$$\n",
    "X = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dots, (x^{(N)}, y^{(N)}) \\in \\mathbb{R}^d \\times \\{-1, 1\\}\\}.\n",
    "$$\n",
    "\n",
    "W przypadku klasyfikatora liniowego granica decyzyjna to hiperpłaszczyzna opisana równaniem:\n",
    "\n",
    "$$\n",
    "w^T x + w_0 = 0,\n",
    "$$\n",
    "\n",
    "gdzie \\(w = [w_1, \\dots, w_d]^T\\).\n",
    "\n",
    "Funkcja decyzyjna, na podstawie której dokonuje się klasyfikacji, jest zdefiniowana jako:\n",
    "\n",
    "$$\n",
    "h(x) =\n",
    "\\begin{cases}\n",
    "1, & \\text{jeśli } w^T x + w_0 \\geq 0, \\\\\n",
    "-1, & \\text{jeśli } w^T x + w_0 < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Maszyna wektorów nośnych pozwala rozszerzyć ten model, umożliwiając klasyfikację danych, które nie są liniowo separowalne, dzięki odpowiednim transformacjom przestrzeni i użyciu miękkiego marginesu, jeśli jest to konieczne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margines w klasyfikacji\n",
    "\n",
    "W kontekście klasyfikacji za pomocą ustalonej granicy decyzyjnej można zdefiniować tzw. margines:\n",
    "\n",
    "$$\n",
    "\\epsilon = \\min_{j=1,\\dots,N} \\epsilon^{(j)},\n",
    "$$\n",
    "\n",
    "gdzie \\(\\epsilon^{(j)}\\) oznacza odległość obiektu \\(x^{(j)}\\) od hiperpłaszczyzny rozdzielającej.\n",
    "\n",
    "Odległość \\(\\epsilon^{(j)}\\) dla obiektu \\(x^{(j)}\\) odpowiada długości wektora prostopadłego do hiperpłaszczyzny i łączącego punkt \\(x^{(j)}\\) z tą hiperpłaszczyzną. Korzystając z metody mnożników Lagrange’a, można udowodnić następujące twierdzenie dotyczące odległości punktu od hiperpłaszczyzny:\n",
    "\n",
    "---\n",
    "\n",
    "**Twierdzenie o odległości punktu od hiperpłaszczyzny**  \n",
    "Dla punktu \\(x^{(j)} = (x_1^{(j)}, x_2^{(j)}, \\dots, x_d^{(j)})\\) w przestrzeni euklidesowej oraz hiperpłaszczyzny opisanej równaniem:\n",
    "\n",
    "$$\n",
    "w^T x + w_0 = 0, \n",
    "$$\n",
    "\n",
    "gdzie \\(w^T = [w_1, \\dots, w_d]\\), odległość punktu \\(x^{(j)}\\) od hiperpłaszczyzny wynosi:\n",
    "\n",
    "$$\n",
    "\\epsilon^{(j)} = \\frac{|w^T x^{(j)} + w_0|}{||w||}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Z funkcji decyzyjnej \\(h_{w, w_0}(x)\\) wynika, że:\n",
    "- Dla punktów należących do klasy \\(y = 1\\), zachodzi \\(w^T x^{(j)} + w_0 \\geq 0\\),\n",
    "- Dla punktów należących do klasy \\(y = -1\\), zachodzi \\(w^T x^{(j)} + w_0 < 0\\).\n",
    "\n",
    "Dlatego odległość punktu \\(x^{(j)}\\) od hiperpłaszczyzny można zapisać jako:\n",
    "\n",
    "$$\n",
    "\\epsilon^{(j)} = y^{(j)} \\frac{(w^T x^{(j)} + w_0)}{||w||}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Wyznaczanie marginesu\n",
    "\n",
    "Wyznaczenie marginesu polega na znalezieniu takiej hiperpłaszczyzny, która maksymalizuje odległość od najbliższych obiektów z każdej klasy. Te najbliższe obiekty nazywane są **wektorami nośnymi** (ang. *support vectors*). Są one kluczowe dla określenia granicy decyzyjnej, ponieważ to one „wspierają” hiperpłaszczyznę i wyznaczają maksymalny margines między obiektami należącymi do różnych klas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasyfikator maksymalizujący margines (Maximal Margin Classifier, MMC)\n",
    "\n",
    "Celem algorytmu maksymalizującego margines jest znalezienie równania hiperpłaszczyzny, która najlepiej rozdziela dane na klasy. Kluczowym założeniem algorytmu jest maksymalizacja marginesu, czyli odległości pomiędzy granicą decyzyjną a najbliższymi obiektami treningowymi dla danego zbioru \\(X\\). Problem optymalizacyjny można sformułować jako:\n",
    "\n",
    "$$\n",
    "\\max \\epsilon_{w, w_0}\n",
    "$$\n",
    "\n",
    "pod warunkiem, że dla \\(j = 1, \\dots, N\\):\n",
    "\n",
    "$$\n",
    "\\epsilon^{(j)} = y^{(j)} \\left( \\frac{w^T}{||w||} x^{(j)} + \\frac{w_0}{||w||} \\right) \\geq \\epsilon,\n",
    "$$\n",
    "\n",
    "Ten zapis gwarantuje spełnienie warunku z definicji marginesu, czyli:\n",
    "\n",
    "$$\n",
    "\\epsilon = \\min_{j=1, \\dots, N} \\epsilon^{(j)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Uwaga  \n",
    "Koncepcja maksymalizacji marginesu opiera się na założeniu, że im dalej obiekt znajduje się od granicy decyzyjnej, tym bardziej pewna jest jego klasyfikacja.\n",
    "\n",
    "---\n",
    "\n",
    "### Przekształcenie problemu optymalizacyjnego\n",
    "Powyższy warunek można zapisać jako:\n",
    "\n",
    "$$\n",
    "y^{(j)} (w^T x^{(j)} + w_0) \\geq \\epsilon \\cdot ||w||.\n",
    "$$\n",
    "\n",
    "Ustalając \\(\\epsilon \\cdot ||w|| = E\\), problem optymalizacyjny przyjmuje postać:\n",
    "\n",
    "$$\n",
    "\\max_{w, w_0} \\frac{E}{||w||}\n",
    "$$\n",
    "\n",
    "pod warunkiem, że dla \\(j = 1, \\dots, N\\):\n",
    "\n",
    "$$\n",
    "y^{(j)} (w^T x^{(j)} + w_0) \\geq E.\n",
    "$$\n",
    "\n",
    "Przeskalowanie \\(w\\) i \\(w_0\\) nie zmienia wyniku optymalizacji, dlatego zakładamy \\(E = 1\\). Wtedy problem można uprościć do:\n",
    "\n",
    "$$\n",
    "\\max_{w, w_0} \\frac{1}{||w||},\n",
    "$$\n",
    "\n",
    "co jest równoważne minimalizacji:\n",
    "\n",
    "$$\n",
    "\\min ||w|| \\quad \\text{lub} \\quad \\min \\frac{1}{2} ||w||^2.\n",
    "$$\n",
    "\n",
    "Ostateczny problem optymalizacyjny:\n",
    "\n",
    "$$\n",
    "\\min_{w, w_0} \\frac{1}{2} ||w||^2\n",
    "$$\n",
    "\n",
    "pod warunkiem, że dla \\(j = 1, \\dots, N\\):\n",
    "\n",
    "$$\n",
    "y^{(j)} (w^T x^{(j)} + w_0) - 1 \\geq 0.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Wypukłość problemu  \n",
    "Jest to kwadratowy problem optymalizacyjny, a jego funkcja celu ma dokładnie jedno minimum, co oznacza, że uzyskane rozwiązanie jest globalne. Rozwiązanie problemu można znaleźć za pomocą metody mnożników Lagrange’a, co prowadzi do wyznaczenia optymalnej hiperpłaszczyzny maksymalizującej margines:\n",
    "\n",
    "$$\n",
    "w^T x + w_0 = 0,\n",
    "$$\n",
    "\n",
    "gdzie:\n",
    "\n",
    "$$\n",
    "w = \\sum_{j \\in S} \\alpha_j y^{(j)} x^{(j)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_0 = \\frac{w^T x^* - w^T x^{**}}{2}, \\quad j \\in S,\n",
    "$$\n",
    "\n",
    "a \\(S\\) to zbiór wektorów nośnych. Parametr \\(w_0\\) wyznacza położenie hiperpłaszczyzny i umiejscawia ją między hiperpłaszczyznami opartymi na wektorach nośnych dla obu klas.\n",
    "\n",
    "---\n",
    "\n",
    "### Reguła decyzyjna\n",
    "Reguła klasyfikacyjna dla nowego obiektu \\(x\\) przyjmuje postać:\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sgn} \\left( \\sum_{j \\in S} \\alpha_j y^{(j)} x^{(j)} x + w_0 \\right).\n",
    "$$\n",
    "\n",
    "Rysunek przedstawia przykładową hiperpłaszczyznę oraz margines dla zbioru danych z dwoma atrybutami i etykietami decyzyjnymi należącymi do zbioru \\(\\{-1, 1\\}\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasyfikator wektorów nośnych (ang. Support Vector Classifier, SVC)\n",
    "\n",
    "W większości przypadków dane nie są idealnie liniowo separowalne, co oznacza, że nie istnieje hiperpłaszczyzna, która rozdzieliłaby wszystkie obiekty bezbłędnie. W takich sytuacjach algorytm maksymalizacji marginesu nie znajduje rozwiązania. W odpowiedzi na te ograniczenia stosuje się **klasyfikator wektorów nośnych**, który pozwala znaleźć hiperpłaszczyznę rozdzielającą większość obiektów, a nie wszystkie. Kluczową koncepcją tego algorytmu jest wprowadzenie **marginesu miękkiego** (ang. soft margin).\n",
    "\n",
    "Margines miękki umożliwia wyznaczenie hiperpłaszczyzny, która toleruje pewne błędy klasyfikacyjne. W tym celu wprowadza się:\n",
    "- **Parametr regulacyjny \\(C \\geq 0\\)**, który kontroluje liczbę dopuszczalnych błędów,\n",
    "- **Parametry luzu \\(\\delta_j\\)** (ang. slack parameters), które pozwalają na naruszenie marginesu przez niektóre obiekty.\n",
    "\n",
    "Zapis problemu optymalizacyjnego prowadzącego do znalezienia hiperpłaszczyzny można wyrazić jako:\n",
    "\n",
    "$$\n",
    "\\max_{w, w_0, \\delta_1, \\dots, \\delta_N} \\epsilon, \n",
    "$$\n",
    "\n",
    "przy warunkach:\n",
    "\n",
    "$$\n",
    "y^{(j)} (w^T x^{(j)} + w_0) \\geq \\epsilon (1 - \\delta_j),\n",
    "$$\n",
    "\n",
    "$$\n",
    "||w|| = 1,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_j \\geq 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\delta_j \\leq C.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretacja parametrów\n",
    "\n",
    "Po rozwiązaniu powyższego problemu optymalizacyjnego otrzymujemy hiperpłaszczyznę, która jest wykorzystywana do klasyfikacji nowych obiektów. Podobnie jak w klasyfikatorze maksymalizującym margines, klasyfikacja odbywa się na podstawie znaku wartości hiperpłaszczyzny w punkcie reprezentującym dany obiekt.\n",
    "\n",
    "**Parametry \\(\\delta_j\\)** informują o położeniu obiektów względem hiperpłaszczyzny i marginesu:\n",
    "- Jeśli \\(\\delta_j = 0\\), obiekt \\(x^{(j)}\\) znajduje się po właściwej stronie marginesu.\n",
    "- Jeśli \\(0 < \\delta_j \\leq 1\\), obiekt narusza margines.\n",
    "- Jeśli \\(\\delta_j > 1\\), obiekt znajduje się po złej stronie hiperpłaszczyzny.\n",
    "\n",
    "**Parametr regulacyjny \\(C\\)** ogranicza sumę wszystkich \\(\\delta_j\\), kontrolując maksymalną liczbę naruszeń marginesu i błędów klasyfikacyjnych:\n",
    "- Dla \\(C = 0\\), żadnemu obiektowi nie wolno naruszyć marginesu – problem sprowadza się do klasyfikatora maksymalizującego margines.\n",
    "- Dla \\(C > 0\\), dopuszczalna jest maksymalnie suma \\(C\\) naruszeń marginesu i błędów klasyfikacyjnych.\n",
    "\n",
    "Wraz ze wzrostem wartości \\(C\\), klasyfikator staje się mniej wrażliwy na naruszanie marginesu i błędne klasyfikacje, co prowadzi do bardziej elastycznego modelu. Natomiast mniejsze wartości \\(C\\) powodują, że klasyfikator działa bardziej restrykcyjnie.\n",
    "\n",
    "---\n",
    "\n",
    "### Dobór parametru \\(C\\)\n",
    "\n",
    "Parametr \\(C\\) jest hiperparametrem, który należy ustalić na etapie konfiguracji modelu. Jego wartość często dobiera się za pomocą metody sprawdzianu krzyżowego, co pozwala znaleźć optymalne ustawienie dostosowane do konkretnego zbioru danych. Wartości \\(C\\) mogą być różne w zależności od charakterystyki danych i wymagań dotyczących dokładności modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maszyna wektorów nośnych (ang. Support Vector Machine, SVM)\n",
    "\n",
    "Aby zrozumieć działanie maszyny wektorów nośnych, warto najpierw omówić sposób przekształcania klasyfikatora liniowego w klasyfikator z nieliniową granicą decyzyjną. Automatyzacja tego procesu odbywa się za pomocą maszyny wektorów nośnych.\n",
    "\n",
    "Załóżmy, że \\( X = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dots, (x^{(N)}, y^{(N)}) \\in \\mathbb{R}^d \\times \\{-1, 1\\}\\} \\) to zbiór danych z nieliniową granicą między klasami. W takim przypadku hiperpłaszczyzna utworzona za pomocą narzędzi opisanych wcześniej nie będzie efektywnym klasyfikatorem. Dla danych nieliniowo separowalnych można zastosować przekształcenie do przestrzeni o wyższym wymiarze poprzez dodanie nowych cech, które są funkcjami oryginalnych zmiennych. Często stosowane są funkcje wielomianowe, np. \\( x_1^2, x_2^2, \\dots, x_d^2 \\).\n",
    "\n",
    "Granica decyzyjna w wyższej przestrzeni może być opisana równaniem:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^d v_i x_i + \\sum_{i=1}^d w_i x_i^2 + w_0 = 0.\n",
    "$$\n",
    "\n",
    "W tym przypadku problem optymalizacyjny dla maszyny wektorów nośnych przyjmuje postać:\n",
    "\n",
    "$$\n",
    "\\max_{\\epsilon, w, v, w_0, \\delta_1, \\dots, \\delta_N} \\epsilon,\n",
    "$$\n",
    "\n",
    "przy warunkach:\n",
    "\n",
    "$$\n",
    "y^{(j)} \\left( \\sum_{i=1}^d v_i (x_i^{(j)})^2 + \\sum_{i=1}^d w_i x_i^{(j)} + w_0 \\right) \\geq \\epsilon (1 - \\delta_j),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^d v_i + \\sum_{i=1}^d w_i = 1,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_j \\geq 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\delta_j \\leq C.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Przekształcenie do przestrzeni wyższego wymiaru\n",
    "\n",
    "Wprowadzenie dodatkowych cech, takich jak wielomiany czy interakcje między zmiennymi (\\( x_i x_j \\), gdzie \\( i \\neq j \\)), pozwala na lepsze dopasowanie granicy decyzyjnej. Jednak zbyt duża liczba cech może prowadzić do przekleństwa wymiarowości i przetrenowania modelu, co skutkuje niską zdolnością generalizacji na nowych danych.\n",
    "\n",
    "Można uogólnić tę ideę, stosując funkcję przekształcającą \n",
    "\n",
    "$$ \n",
    "\\phi : \\mathbb{R}^d \\to \\mathbb{R}^m\n",
    "$$\n",
    "\n",
    ", która przenosi dane do przestrzeni o wyższym wymiarze (\\( m > d \\)). W nowej przestrzeni problem optymalizacyjny zapisuje się jako:\n",
    "\n",
    "$$\n",
    "\\max_{w, w_0, \\delta_1, \\dots, \\delta_N} \\epsilon,\n",
    "$$\n",
    "\n",
    "przy warunkach:\n",
    "\n",
    "$$\n",
    "y^{(j)} \\left(w^T \\phi(x^{(j)}) + w_0\\right) \\geq \\epsilon (1 - \\delta_j),\n",
    "$$\n",
    "\n",
    "$$\n",
    "||w|| = 1,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_j \\geq 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\delta_j \\leq C.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Sztuczka jądra (ang. Kernel Trick)\n",
    "\n",
    "Zamiast bezpośrednio przekształcać dane za pomocą funkcji \\( \\phi(x) \\), można wykorzystać funkcję jądra \\( K(x, x') \\), która pozwala obliczać iloczyn skalarny \\( \\phi(x) \\cdot \\phi(x') \\) bez wyraźnego znajomości funkcji \\( \\phi \\). Funkcja jądra jest symetryczna i nieujemnie określona. Przykłady popularnych jąder to:\n",
    "\n",
    "1. **Jądro liniowe**: \\( K(x, x') = x^T x' \\),\n",
    "2. **Jądro wielomianowe**: \\( K(x, x') = (x^T x' + 1)^m \\), gdzie \\( m \\) to stopień wielomianu,\n",
    "3. **Jądro gaussowskie (RBF)**: \\( K(x, x') = \\exp\\left(-\\frac{||x - x'||^2}{2\\sigma^2}\\right) \\), gdzie \\( \\sigma > 0 \\).\n",
    "\n",
    "Dzięki sztuczce jądra reguła decyzyjna maszyny wektorów nośnych ma postać:\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sgn} \\left( \\sum_{j \\in S} \\alpha_j y^{(j)} K(x^{(j)}, x) + w_0 \\right).\n",
    "$$\n",
    "\n",
    "To podejście pozwala uzyskać optymalną hiperpowierzchnię dyskryminacyjną w przestrzeni wyższych wymiarów bez wyraźnego definiowania przekształcenia \\( \\phi(x) \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maszyna wektorów nośnych w klasyfikacji wieloklasowej\n",
    "\n",
    "Dotychczas omawiane algorytmy wykorzystujące wektory nośne odnosiły się jedynie do klasyfikacji binarnej, w której atrybut decyzyjny przyjmował jedną z dwóch wartości, \\( Y = \\{-1, 1\\} \\). Jednak maszyna wektorów nośnych może być również używana w zadaniach klasyfikacji wieloklasowej. Najczęściej w tym celu stosuje się podejście polegające na wykorzystaniu wielu klasyfikatorów binarnych, w ramach których zestawia się pary klas lub jedną klasę przeciwko wszystkim pozostałym. Tego typu metody to odpowiednio **jeden kontra jeden** (ang. *one-versus-one*) oraz **jeden kontra wszystkie** (ang. *one-versus-all*). \n",
    "\n",
    "W literaturze można znaleźć także inne strategie łączenia binarnych maszyn wektorów nośnych, które pozwalają na skuteczną klasyfikację wieloklasową (Vapnik, 1998)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kroki algorytmu: Jeden kontra wszystkie\n",
    "\n",
    "Niech \n",
    "$$ \n",
    "X = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dots, (x^{(N)}, y^{(N)}) \\in \\mathbb{R}^d \\times \\{1, 2, \\dots, p\\} \\} \n",
    "$$ \n",
    "będzie zbiorem treningowym, gdzie wartości atrybutu decyzyjnego należą do \\( p \\)-elementowego zbioru klas (\\( p > 2 \\)).\n",
    "\n",
    "1. Ustaw \\( i := 1 \\).\n",
    "2. Podziel zbiór:\n",
    "   $$ A = \\{(x^{(j)}, y^{(j)}) : y^{(j)} = i\\}, $$\n",
    "   $$ B = X - A. $$\n",
    "3. Przypisz nowe wartości atrybutu decyzyjnego:\n",
    "   - W zbiorze \\( A \\) przypisz wartość \\( 1 \\),\n",
    "   - W zbiorze \\( B \\) przypisz wartość \\( -1 \\).\n",
    "4. Traktuj \\( A \\cup B \\) jako zbiór treningowy i zbuduj maszynę wektorów nośnych, uzyskując regułę \\( f_i(x) \\).\n",
    "5. Sprawdź liczbę kroków:\n",
    "   - Jeśli \\( i < p \\), zwiększ \\( i \\) o 1 i wróć do kroku 2,\n",
    "   - Jeśli \\( i = p \\), przejdź do kroku 6.\n",
    "6. Określ regułę decyzyjną klasyfikacji wieloklasowej:\n",
    "   $$\n",
    "   f(x) = \\arg\\max_{i=1, \\dots, p} f_i(x).\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Kroki algorytmu: Jeden kontra jeden\n",
    "\n",
    "Niech \n",
    "$$ \n",
    "X = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dots, (x^{(N)}, y^{(N)}) \\in \\mathbb{R}^d \\times \\{1, 2, \\dots, p\\} \\} \n",
    "$$ \n",
    "będzie zbiorem treningowym, gdzie wartości atrybutu decyzyjnego należą do \\( p \\)-elementowego zbioru klas (\\( p > 2 \\)).\n",
    "\n",
    "1. Zdefiniuj zbiory dla każdej klasy:\n",
    "   $$\n",
    "   Z_i = \\{(x^{(j)}, y^{(j)}) : y^{(j)} = i\\} \\quad \\text{dla } i = 1, 2, \\dots, p.\n",
    "   $$\n",
    "2. Utwórz wszystkie możliwe pary klas \\( \\{Z_k, Z_l\\} \\), gdzie \\( k \\neq l \\). Liczba takich kombinacji wynosi \n",
    "   $$\n",
    "   \\binom{p}{2}.\n",
    "   $$\n",
    "   Dla każdej pary:\n",
    "   - Połącz zbiory \\( Z_k \\cup Z_l \\), aby uzyskać nowe zbiory treningowe \\( G_n \\), gdzie \n",
    "   $$\n",
    "   n = 1, 2, \\dots, \\binom{p}{2}.\n",
    "   $$\n",
    "   - Każda z \\( p \\) klas występuje w \\( p-1 \\) parach.\n",
    "3. Ustaw \\( n := 1 \\).\n",
    "4. Traktuj zbiór \\( G_n \\) jako treningowy i zbuduj maszynę wektorów nośnych, uzyskując regułę \\( f_n(x) \\).\n",
    "   - **Uwaga:** Wartości atrybutu decyzyjnego w zbiorze \\( G_n \\) muszą być zmienione na \\( 1 \\) i \\( -1 \\), ale po zakończeniu klasyfikacji należy przywrócić oryginalne oznaczenia klas \\( k \\) i \\( l \\).\n",
    "5. Sprawdź liczbę kroków:\n",
    "   - Jeśli \n",
    "   $$\n",
    "   n < \\binom{p}{2},\n",
    "   $$\n",
    "   zwiększ \\( n \\) o 1 i wróć do kroku 4,\n",
    "   - Jeśli \n",
    "   $$\n",
    "   n = \\binom{p}{2},\n",
    "   $$\n",
    "   przejdź do kroku 6.\n",
    "6. Ustal regułę decyzyjną klasyfikacji wieloklasowej:\n",
    "   - Przypisz nowy obiekt do klasy, która została najczęściej wskazana przez \n",
    "   $$\n",
    "   \\binom{p}{2}\n",
    "   $$\n",
    "   klasyfikatory binarne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład: Automatyczny dobór leków w oparciu o parametry fizjologiczne pacjenta\n",
    "\n",
    "Dane dostępne pod adresem: [https://www.kaggle.com/datasets/prathamtripathi/drug-classification](https://www.kaggle.com/datasets/prathamtripathi/drug-classification) zostaną wykorzystane do zademonstrowania działania algorytmu SVM. Zbiór danych zawiera sześć zmiennych opisujących pacjentów:\n",
    "\n",
    "1. **Age** – wiek pacjenta,\n",
    "2. **Sex** – płeć pacjenta,\n",
    "3. **Blood Pressure Levels (BP)** – poziom ciśnienia krwi,\n",
    "4. **Cholesterol Levels** – poziom cholesterolu,\n",
    "5. **Na to Potassium Ratio** – stosunek stężenia sodu do stężenia potasu we krwi,\n",
    "6. **Drug type** – rodzaj przepisanego leku.\n",
    "\n",
    "Analizując wartości pięciu pierwszych zmiennych (czyli parametrów fizjologicznych), można z dużym prawdopodobieństwem przewidzieć, który lek będzie odpowiedni dla danego pacjenta.\n",
    "\n",
    "#### Przygotowanie danych\n",
    "\n",
    "Przed rozpoczęciem uczenia modelu, dane muszą zostać odpowiednio przygotowane:\n",
    "\n",
    "1. **Wczytanie danych**: Dane zostaną załadowane do struktury `DataFrame`.\n",
    "2. **Czyszczenie danych**: Wiersze z brakującymi wartościami zostaną usunięte.\n",
    "3. **Kodowanie zmiennych kategorialnych**: Zarówno etykiety, jak i niektóre parametry są zmiennymi kategorialnymi. \n",
    "   - Do zakodowania parametrów zostanie wykorzystana funkcja `OrdinalEncoder`.\n",
    "   - Do zakodowania etykiet (rodzaj leku) zostanie użyta funkcja `LabelEncoder`.\n",
    "4. **Podział danych**: Dane zostaną podzielone na zbiór uczący i testowy.\n",
    "5. **Standaryzacja danych**: Dane numeryczne zostaną przeskalowane, aby zapewnić, że wszystkie cechy będą miały ten sam zakres wartości, co ułatwi trening modelu. \n",
    "\n",
    "Tak przygotowane dane będą gotowe do zastosowania algorytmu SVM, który pozwoli przewidzieć odpowiedni lek na podstawie parametrów pacjenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Wczytanie danych i usunięcie brakujących wartości\n",
    "df = pd.read_csv('drug200.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Utworzenie wektora etykiet i macierzy parametrów\n",
    "columns = df.columns\n",
    "data = df[columns[:-1]].to_numpy()\n",
    "labels = df['Drug'].to_numpy()\n",
    "\n",
    "# Zakodowanie zmiennych kategorialnych na liczby\n",
    "enc = OrdinalEncoder()\n",
    "data = enc.fit_transform(data)\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "labels = label_enc.fit_transform(labels)\n",
    "\n",
    "# Podział danych na zbiór uczący i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels,\n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standaryzacja danych\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnym krokiem jest analiza liczebności poszczególnych klas. To, czy dane są równomiernie rozłożone między klasy, czy też mamy do czynienia z ich nierównowagą, wpływa na wybór metryk sukcesu, które najlepiej ocenią skuteczność wytrenowanego klasyfikatora.\n",
    "\n",
    "Do zbadania liczebności klas zostanie wykorzystana funkcja `Counter` z biblioteki `collections`, która umożliwia zliczenie liczby obiektów w każdej klasie. Dodatkowo zostanie wygenerowany wykres, który graficznie przedstawi rozkład liczebności między klasami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Wyświetlenie liczebności poszczególnych klas\n",
    "print(Counter(labels))\n",
    "\n",
    "# Wizualizacja rozkładu klas\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(\n",
    "    labels,\n",
    "    bins=np.arange(len(np.unique(labels)) + 1) - 0.5,\n",
    "    rwidth=0.8\n",
    ")\n",
    "\n",
    "# Przygotowanie etykiet osi X\n",
    "drugs_names = label_enc.inverse_transform(np.unique(labels))\n",
    "plt.xticks(np.unique(labels), drugs_names, ha='center', fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Wyświetlenie wykresu\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak można zauważyć, rozkład liczebności klas jest wyraźnie nierównomierny – leki B i C pojawiają się jedynie 16 razy, podczas gdy lek Y występuje aż 91 razy. Taka nierównowaga powinna zostać uwzględniona podczas procesu trenowania i oceny modelu, szczególnie w trakcie optymalizacji hiperparametrów. Wybór niewłaściwej metryki może prowadzić do stworzenia modelu o bardzo niskiej czułości dla mniej licznych klas.\n",
    "\n",
    "Na początek zostanie przeprowadzony trening modelu z domyślnymi wartościami hiperparametrów. Do inicjalizacji klasyfikatora wektorów nośnych wykorzystana zostanie klasa `SVC` dostępna w bibliotece scikit-learn. Proces treningu i dokonywania predykcji na danych testowych zostanie zrealizowany przy użyciu metod `fit` oraz `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Inicjalizacja i trening modelu SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predykcja na zbiorze testowym\n",
    "preds = svm.predict(X_test_scaled)\n",
    "\n",
    "# Wyświetlenie raportu klasyfikacji i macierzy pomyłek\n",
    "print(classification_report(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza wyników i optymalizacja modelu SVC\n",
    "\n",
    "Przeglądając wyniki klasyfikacji wygenerowane za pomocą funkcji `classification_report`, można zauważyć, że najniższa skuteczność dotyczy klasyfikacji leku B. Wartość czułości dla tej klasy wynosi zaledwie 0,5, co oznacza, że połowa obiektów należących do tej klasy w zbiorze testowym została błędnie zaklasyfikowana. Warto jednak podkreślić, że z uwagi na bardzo małą liczebność tej klasy w zbiorze testowym znalazły się jedynie cztery obiekty. W związku z tym wszelkie wnioski dotyczące wyników klasyfikacji dla tej klasy powinny być formułowane z dużą ostrożnością, ponieważ parametry cechujące te obiekty mogą nie być reprezentatywne dla całej populacji.\n",
    "\n",
    "Kolejnym krokiem jest optymalizacja hiperparametrów modelu SVC. Proces ten obejmie cztery kluczowe hiperparametry:\n",
    "\n",
    "1. **C** – parametr regularyzacji, który kontroluje siłę karania za błędy klasyfikacji;\n",
    "2. **max_iter** – maksymalna liczba iteracji, podczas których algorytm dąży do znalezienia optymalnych wag. W przypadku braku zbieżności proces optymalizacji zostaje przerwany, a przyjmowany jest najlepszy dotychczasowy wynik;\n",
    "3. **kernel** – funkcja jądra stosowana w algorytmie SVM;\n",
    "4. **degree** – stopień wielomianu w przypadku użycia wielomianowej funkcji jądra.\n",
    "\n",
    "Pełną listę możliwych hiperparametrów modelu można znaleźć w dokumentacji klasy SVC: [sklearn.svm.SVC documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "W trakcie optymalizacji jako metrykę ewaluacyjną wybrano wskaźnik **F1**, który uwzględnia zarówno precyzję, jak i czułość modelu. Jest to odpowiedni wybór w przypadku pracy z niezrównoważonymi zbiorami danych. Ponieważ dane zawierają więcej niż dwie klasy, konieczne było określenie metody obliczania średniej wartości F1. Przy dużych dysproporcjach liczebności klas zaleca się stosowanie średniej ważonej, dlatego zdecydowano się na użycie tej metody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Definicja metryki scoringowej\n",
    "scoring = {'F1': make_scorer(f1_score, average='weighted')}\n",
    "\n",
    "# Funkcja celu\n",
    "def objective(trial, model, get_space, X, y):\n",
    "    model_space = get_space(trial)\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(\n",
    "        mdl, X, y,\n",
    "        scoring=scoring,\n",
    "        cv=StratifiedKFold(n_splits=5),\n",
    "        return_train_score=True\n",
    "    )\n",
    "    return np.mean(scores['test_F1'])\n",
    "\n",
    "# Przestrzeń hiperparametrów\n",
    "def get_space(trial):\n",
    "    space = {\n",
    "        \"C\": trial.suggest_uniform(\"C\", 0, 2),\n",
    "        \"max_iter\": trial.suggest_int(\"max_iter\", 100, 1000),\n",
    "        \"kernel\": trial.suggest_categorical(\n",
    "            \"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "        ),\n",
    "        \"degree\": trial.suggest_int(\"degree\", 2, 5)\n",
    "    }\n",
    "    return space\n",
    "\n",
    "# Liczba prób optymalizacji\n",
    "trials = 120\n",
    "model = SVC\n",
    "\n",
    "# Optymalizacja\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(\n",
    "    lambda x: objective(x, model, get_space, X_train_scaled, y_train),\n",
    "    n_trials=trials\n",
    ")\n",
    "\n",
    "# Wyświetlenie najlepszych wartości hiperparametrów\n",
    "print('params:', study.best_params)\n",
    "\n",
    "# Trening modelu z optymalnymi hiperparametrami\n",
    "svm = model(**study.best_params)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Ewaluacja modelu\n",
    "preds = svm.predict(X_test_scaled)\n",
    "print(classification_report(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po przeprowadzeniu optymalizacji hiperparametrów modelu zaobserwowano poprawę wyników na zbiorze testowym. Lek A został zaklasyfikowany poprawnie we wszystkich przypadkach, podczas gdy wcześniej jeden obiekt był błędnie przypisany. Dodatkowo poprawie uległy wyniki dla najmniej licznych klas – lek B został poprawnie sklasyfikowany w trzech z czterech przypadków, co oznacza wzrost czułości modelu o 25 punktów procentowych. Jedynym wyjątkiem był lek X, gdzie po optymalizacji jeden obiekt został błędnie zaklasyfikowany, co pogorszyło wyniki dla tej klasy. Mimo to, biorąc pod uwagę ogólną poprawę dokładności klasyfikacji oraz wzrost skuteczności dla pozostałych klas, można stwierdzić, że optymalizacja hiperparametrów przyczyniła się do uzyskania modelu o lepszych zdolnościach klasyfikacyjnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie: Klasyfikacja gwiazd na podstawie klasy jasności\n",
    "\n",
    "Dane dotyczące gwiazd różnych klas jasności znajdują się pod adresem: [https://www.kaggle.com/datasets/deepu1109/star-dataset](https://www.kaggle.com/datasets/deepu1109/star-dataset). Każda gwiazda jest opisana przez sześć parametrów:\n",
    "\n",
    "1. **Absolute Temperature (K)** – temperatura w kelwinach,\n",
    "2. **Relative Luminosity (L/Lo)** – jasność względna w odniesieniu do jasności Słońca,\n",
    "3. **Relative Radius (R/Ro)** – promień względny w odniesieniu do promienia Słońca,\n",
    "4. **Absolute Magnitude (Mv)** – absolutna wielkość gwiazdowa,\n",
    "5. **Star Color** – kolor gwiazdy,\n",
    "6. **Spectral Class** – typ widmowy.\n",
    "\n",
    "Klasy jasności gwiazd zapisane w kolumnie `star type` są oznaczone następującymi wartościami:\n",
    "\n",
    "- **0** – brązowy karzeł (*brown dwarf*),\n",
    "- **1** – czerwony karzeł (*red dwarf*),\n",
    "- **2** – biały karzeł (*white dwarf*),\n",
    "- **3** – gwiazda ciągu głównego (*main sequence*),\n",
    "- **4** – nadolbrzym (*supergiant*),\n",
    "- **5** – hiperolbrzym (*hypergiant*).\n",
    "\n",
    "---\n",
    "\n",
    "### Instrukcja\n",
    "\n",
    "Aby przeprowadzić klasyfikację gwiazd na podstawie ich klasy jasności, wykonaj poniższe kroki:\n",
    "\n",
    "1. **Wczytanie danych**: Pobierz dane i załaduj je do struktury `DataFrame`.\n",
    "2. **Przygotowanie danych**:\n",
    "   - Usuń brakujące wartości.\n",
    "   - Zakoduj zmienne kategorialne na zmienne numeryczne.\n",
    "   - Podziel dane na zbiór uczący i testowy.\n",
    "   - Ustandaryzuj dane numeryczne.\n",
    "3. **Trening i ewaluacja modelu bazowego**:\n",
    "   - Przeprowadź trening klasyfikatora SVC z domyślnymi wartościami hiperparametrów.\n",
    "   - Dokonaj ewaluacji wyników na zbiorze testowym.\n",
    "4. **Optymalizacja hiperparametrów**:\n",
    "   - Przeprowadź optymalizację hiperparametrów modelu, wybierając odpowiednią metrykę ewaluacyjną.\n",
    "   - Wykonaj ponowny trening modelu z optymalnymi wartościami hiperparametrów i przeprowadź ewaluację.\n",
    "   - Ustal, która metryka ewaluacyjna (np. F1, ROC AUC) najlepiej pasuje do tego zadania i uzasadnij swój wybór.\n",
    "5. **Analiza wyników**:\n",
    "   - Porównaj wyniki modelu przed i po optymalizacji.\n",
    "   - Oceń, czy optymalizacja poprawiła zdolności predykcyjne modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
